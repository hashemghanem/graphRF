{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omPBbM1m2tEa"
   },
   "source": [
    "# Gaussian kernel Approximation\n",
    "### In this code we approximate the Gaussian kernel of two graphs by the dot product of two k-random features consinus vectors, each belongs to the corresponding graph\n",
    "### Datasets used: mutag, SBM or DD dataset\n",
    "### Graph sampling technique used in this code: Simple Random Sampling, Random Walk With flyback, simple random walk sampling\n",
    "### Machine learning algo: SVM with kernel\n",
    "\n",
    "## This code can be uploaded on colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install grakel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1614,
     "status": "ok",
     "timestamp": 1584695839638,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "ATWEei5-2tEe",
    "outputId": "eaa3b7ba-d995-4e9d-fcd2-6db4a029a462"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nWe can follow Random features for large-scale kernel machines, A. Rahimi and B. Recht (2007) and \\nsolve a standard ridge regression on a nonlinear mapping of the data to a new feature space of a \\ndifferent dimension.\\n\\nWhen the number of random projections m tends to infinity, the inner product between the projected\\ndata points approximates a kernel function, due to the concentration of measure (Computation with \\ninfinite neural networks, C. Williams, 1998).\\n\\nThe matrix-vector multiplication outputs a (1×m) vector, complex-valued. This is followed by the \\nelement-wise non-linearity |.|2 and the quantization due to analog to digital conversion. Finally,\\nthe output of the OPU is y a column vector of size (1×m) of type uint8. The independence of the \\nentries of the output vector means that the rows of the matrix R are independent.\\n\\nThe OPU requires a binary matrix of type uint8 as input.\\n\\nyou need: 1. encoder\\n          2. opu (random mapping)\\n          3. decoder\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import grakel as gk\n",
    "from grakel.utils import graph_from_networkx\n",
    "from grakel.datasets import fetch_dataset\n",
    "from grakel import Graph\n",
    "from grakel.kernels import ShortestPath\n",
    "from grakel.kernels import GraphletSampling\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import random\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings                                     # from here on, it is from lighton\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "from lightonml.projections.sklearn import OPUMap\n",
    "from lightonml.encoding.base import NoDecoding\n",
    "from lightonml.encoding.base import NoEncoding\n",
    "\n",
    "'''\n",
    "We can follow Random features for large-scale kernel machines, A. Rahimi and B. Recht (2007) and \n",
    "solve a standard ridge regression on a nonlinear mapping of the data to a new feature space of a \n",
    "different dimension.\n",
    "\n",
    "When the number of random projections m tends to infinity, the inner product between the projected\n",
    "data points approximates a kernel function, due to the concentration of measure (Computation with \n",
    "infinite neural networks, C. Williams, 1998).\n",
    "\n",
    "The matrix-vector multiplication outputs a (1×m) vector, complex-valued. This is followed by the \n",
    "element-wise non-linearity |.|2 and the quantization due to analog to digital conversion. Finally,\n",
    "the output of the OPU is y a column vector of size (1×m) of type uint8. The independence of the \n",
    "entries of the output vector means that the rows of the matrix R are independent.\n",
    "\n",
    "The OPU requires a binary matrix of type uint8 as input.\n",
    "\n",
    "you need: 1. encoder\n",
    "          2. opu (random mapping)\n",
    "          3. decoder\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jvh7-0e2tEp"
   },
   "source": [
    "## The next two blocks are the graph_sampling class and the kernel_class \n",
    "### There is no code other than the classes' methods/attributes, so it is recommended that after compiling these two blocks, you go directly to next blocks and then go back when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0_YN-kj2tEq"
   },
   "outputs": [],
   "source": [
    "class graph_sampler:\n",
    "    '''\n",
    "    \n",
    "    This class provides four sampling techniques:\n",
    "    1. simple_random_sampling\n",
    "    2. simple_random_walk_sampling\n",
    "    3. random_walk_flyback_sampling\n",
    "    4. random_walk_induced_graph_sampling\n",
    "    \n",
    "    Note that when initializing a new instance of this class, sampler_type should be the name of the\n",
    "    required technique as specified above, except for simple_random_walk_sampling where sampler_type\n",
    "    must be a tuple (\"random_walk_flyback_sampling\", p_flyback)\n",
    "\n",
    "    In case one wants to preprocess graphlets on-the-fly, a preprocessing function Adj->vector should be provided.\n",
    "    \n",
    "    After you initialize an instance, you can sample your Graph/list of Graphs by calling sample \n",
    "    method ( the last method)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,sampler_type,nodes_num,preprocess=None):\n",
    "        if preprocess is None:\n",
    "            preprocess=lambda x:x.flatten()\n",
    "        self.preprocess=preprocess\n",
    "        self.nodes_num=nodes_num\n",
    "        if(type(sampler_type)==tuple): # ex: this is the case of random_walk_flyback (name, p_flyback)\n",
    "            self.sampler_type=sampler_type[0]\n",
    "            if(sampler_type[0]==\"random_walk_flyback_sampling\"):self.p_flyback=sampler_type[1]\n",
    "        else : self.sampler_type=sampler_type\n",
    "    \n",
    "    def simple_sampling(self,G,nodes_num):     # one simple_random_sample of G\n",
    "        return G.subgraph(random.sample(G.nodes(), nodes_num))\n",
    "\n",
    "    def random_walk_sampling_simple(self,complete_graph, nodes_to_sample):  # also just one sample using this method\n",
    "        T,growth_size=100,2  # number of iterations (attempts to sample the graph)\n",
    "        complete_graph = nx.convert_node_labels_to_integers(complete_graph, 0, 'default', True)\n",
    "        # giving unique id to every node same as built-in function id\n",
    "        for n, data in complete_graph.nodes(data=True):\n",
    "            complete_graph.node[n]['id'] = n\n",
    "\n",
    "        nr_nodes = len(complete_graph.nodes())\n",
    "        upper_bound_nr_nodes_to_sample = nodes_to_sample\n",
    "        index_of_first_random_node = random.randint(0, nr_nodes-1)\n",
    "        sampled_graph = nx.Graph()\n",
    "\n",
    "        sampled_graph.add_node(complete_graph.node[index_of_first_random_node]['id'])\n",
    "\n",
    "        iteration = 1\n",
    "        edges_before_t_iter = 0\n",
    "        curr_node = index_of_first_random_node\n",
    "        while sampled_graph.number_of_nodes() != upper_bound_nr_nodes_to_sample:\n",
    "            edges = [n for n in complete_graph.neighbors(curr_node)]\n",
    "            index_of_edge = random.randint(0, len(edges) - 1)\n",
    "            chosen_node = edges[index_of_edge]\n",
    "            sampled_graph.add_node(chosen_node)\n",
    "            sampled_graph.add_edge(curr_node, chosen_node)\n",
    "            curr_node = chosen_node\n",
    "            iteration = iteration+1\n",
    "\n",
    "            if iteration % T == 0:\n",
    "                if ((sampled_graph.number_of_edges() - edges_before_t_iter) < growth_size):\n",
    "                    curr_node = random.randint(0, nr_nodes-1)\n",
    "                edges_before_t_iter = sampled_graph.number_of_edges()\n",
    "        return sampled_graph\n",
    "    \n",
    "    def random_walk_sampling_with_fly_back(self,complete_graph, nodes_to_sample, fly_back_prob): # returns one sample\n",
    "        growth_size,T=2,100       # number of iterations (attempts to sample the graph)\n",
    "        complete_graph = nx.convert_node_labels_to_integers(complete_graph, 0, 'default', True)\n",
    "        # giving unique id to every node same as built-in function id\n",
    "        for n, data in complete_graph.nodes(data=True):\n",
    "            complete_graph.node[n]['id'] = n\n",
    "\n",
    "        nr_nodes = len(complete_graph.nodes())\n",
    "        upper_bound_nr_nodes_to_sample = nodes_to_sample\n",
    "\n",
    "        index_of_first_random_node = random.randint(0, nr_nodes-1)\n",
    "        sampled_graph = nx.Graph()\n",
    "\n",
    "        sampled_graph.add_node(complete_graph.node[index_of_first_random_node]['id'])\n",
    "\n",
    "        iteration = 1\n",
    "        edges_before_t_iter = 0\n",
    "        curr_node = index_of_first_random_node\n",
    "        while sampled_graph.number_of_nodes() != upper_bound_nr_nodes_to_sample:\n",
    "            edges = [n for n in complete_graph.neighbors(curr_node)]\n",
    "            index_of_edge = random.randint(0, len(edges) - 1)\n",
    "            chosen_node = edges[index_of_edge]\n",
    "            sampled_graph.add_node(chosen_node)\n",
    "            sampled_graph.add_edge(curr_node, chosen_node)\n",
    "            choice = np.random.choice(['prev','neigh'], 1, p=[fly_back_prob,1-fly_back_prob])\n",
    "            if choice == 'neigh':\n",
    "                curr_node = chosen_node\n",
    "            iteration=iteration+1\n",
    "\n",
    "            if iteration % T == 0:\n",
    "                if ((sampled_graph.number_of_edges() - edges_before_t_iter) < growth_size):\n",
    "                    curr_node = random.randint(0, nr_nodes-1)\n",
    "                    print (\"Choosing another random node to continue random walk \")\n",
    "                edges_before_t_iter = sampled_graph.number_of_edges()\n",
    "\n",
    "        return sampled_graph\n",
    "    \n",
    "    \n",
    "    def random_walk_induced_graph_sampling(self, complete_graph, nodes_to_sample):\n",
    "        growth_size,T=2,100       # number of iterations (attempts to sample the graph)\n",
    "        complete_graph = nx.convert_node_labels_to_integers(complete_graph, 0, 'default', True)\n",
    "        # giving unique id to every node same as built-in function id\n",
    "        for n, data in complete_graph.nodes(data=True):\n",
    "            complete_graph.node[n]['id'] = n\n",
    "            \n",
    "        nr_nodes = len(complete_graph.nodes())\n",
    "        upper_bound_nr_nodes_to_sample = nodes_to_sample\n",
    "        index_of_first_random_node = random.randint(0, nr_nodes - 1)\n",
    "\n",
    "        Sampled_nodes = set([complete_graph.node[index_of_first_random_node]['id']])\n",
    "\n",
    "        iteration = 1\n",
    "        nodes_before_t_iter = 0\n",
    "        curr_node = index_of_first_random_node\n",
    "        while len(Sampled_nodes) != upper_bound_nr_nodes_to_sample:\n",
    "            edges = [n for n in complete_graph.neighbors(curr_node)]\n",
    "            index_of_edge = random.randint(0, len(edges) - 1)\n",
    "            chosen_node = edges[index_of_edge]\n",
    "            Sampled_nodes.add(complete_graph.node[chosen_node]['id'])\n",
    "            curr_node = chosen_node\n",
    "            iteration=iteration+1\n",
    "\n",
    "            if iteration % T == 0:\n",
    "                if ((len(Sampled_nodes) - nodes_before_t_iter) < growth_size):\n",
    "                    curr_node = random.randint(0, nr_nodes - 1)\n",
    "                nodes_before_t_iter = len(Sampled_nodes)\n",
    "\n",
    "        sampled_graph = complete_graph.subgraph(Sampled_nodes)\n",
    "\n",
    "        return sampled_graph\n",
    "    \n",
    "    def sample(self,G, samples_num):\n",
    "        for _ in range (samples_num):\n",
    "            if self.sampler_type==\"simple_random_sampling\": sampled_subgraph=self.simple_sampling(G,self.nodes_num)\n",
    "                \n",
    "            elif self.sampler_type==\"simple_random_walk_sampling\":\n",
    "                sampled_subgraph=self.random_walk_sampling_simple(G,self.nodes_num)\n",
    "\n",
    "            elif self.sampler_type==\"random_walk_flyback_sampling\":\n",
    "                sampled_subgraph=self.random_walk_sampling_with_fly_back(G,self.nodes_num,self.p_flyback)\n",
    "                \n",
    "            elif self.sampler_type==\"random_walk_induced_graph_sampling\":\n",
    "                sampled_subgraph=self.random_walk_induced_graph_sampling(G,self.nodes_num)\n",
    "          \n",
    "            adjacency=self.preprocess(nx.to_numpy_array(sampled_subgraph))[:,None] if _==0 \\\n",
    "                      else np.concatenate((adjacency,self.preprocess(nx.to_numpy_array(sampled_subgraph))[:,None]),axis=1)\n",
    "        return adjacency\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJt2mepm2tEx"
   },
   "outputs": [],
   "source": [
    "class feature_map(ABC):\n",
    "    '''\n",
    "    Abstract class for (random) feature mappings.\n",
    "    Ensure that the transform method is implemented.\n",
    "    '''\n",
    "    def __init__(self, input_dim, features_num):\n",
    "        self.input_dim=input_dim\n",
    "        self.output_dim=output_dim\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, A):\n",
    "        '''\n",
    "        In: A (input_dim * n)\n",
    "        Out: B (output_dim * n)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "class Gaussian_random_features(feature_map):\n",
    "    '''\n",
    "    This class affords an approximation of the Gaussian kernel using random features.\n",
    "    When initializing a new instance, you should pass: \n",
    "    sigma: STD of the Gaussian kernel\n",
    "    input_dim, features_num: size of projection matrix\n",
    "    '''\n",
    "    def __init__(self, input_dim, features_num, sigma):\n",
    "        self.proj_mat=sigma*np.random.randn(features_num,input_dim) \n",
    "        self.features_num=features_num\n",
    "\n",
    "    def transform(self, A):\n",
    "        temp = self.proj_mat.dot(A)\n",
    "        return np.concatenate((np.cos(temp),np.sin(temp)))\n",
    "\n",
    "    \n",
    "\n",
    "class Lighton_random_features(feature_map):\n",
    "    '''\n",
    "    This class affords an approximation of the Gaussian kernel using random features.\n",
    "    When initializing a new instance, you should pass: \n",
    "    sigma: STD of the Gaussian kernel\n",
    "    input_dim, features_num: size of projection matrix\n",
    "    '''\n",
    "    def __init__(self, input_dim, features_num):\n",
    "        self.features_num=features_num\n",
    "        self.random_mapping = OPUMap(n_components=features_num)\n",
    "\n",
    "    def transform(self, A):\n",
    "        A=np.uint8(A.T)\n",
    "        train_random_features = self.random_mapping.transform(A)\n",
    "        return train_random_features.astype('float32').T\n",
    "\n",
    "class graphlet_avg_features():\n",
    "    '''\n",
    "    Main class for graphlet (random) feature averaging.\n",
    "    Instanciated with a graph_sampler and a feature_map.\n",
    "    For each graph, graphlet sampling can be done by batch until samples_num is reached (by default, only one batch).\n",
    "    The graphlet size is implicitly contained in sampler and feat_map (of course, they should match)\n",
    "    '''\n",
    "    def __init__(self, samples_num, sampler, feat_map, batch_size=None, verbose=False):\n",
    "        if batch_size is None:\n",
    "            batch_size=samples_num\n",
    "        self.num_batches=int(samples_num/batch_size)\n",
    "        self.samples_num=self.num_batches*batch_size\n",
    "        self.batch_size=batch_size\n",
    "        self.sampler=sampler\n",
    "        self.feat_map=feat_map\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def calc_one_graph(self, G):\n",
    "        for _ in range(self.num_batches):\n",
    "            graphlets=self.sampler.sample(G, self.batch_size) # d*batch_size\n",
    "            random_feature=self.feat_map.transform(graphlets) # m*batch_size\n",
    "            result=random_feature.sum(axis=1) if _==0 \\\n",
    "                  else result + random_feature.sum(axis=1)\n",
    "        return result/self.samples_num\n",
    "\n",
    "    def apply(self, Gnx):\n",
    "        for (i,G) in enumerate(Gnx):\n",
    "            if self.verbose and np.mod(i,10)==0: print('Graph {}/{}'.format(i,len(Gnx)))\n",
    "            res=self.calc_one_graph(G)[:,None] if i==0 \\\n",
    "                else np.concatenate((res,self.calc_one_graph(G)[:,None]),axis=1)\n",
    "        return res\n",
    "    \n",
    "def calc_kernel(G_train, G_test, graphletRF):\n",
    "    random_features_train=graphletRF.apply(G_train)\n",
    "    random_features_test=graphletRF.apply(G_test)\n",
    "    K_train =random_features_train.T.dot(random_features_train)\n",
    "    K_test =random_features_test.T.dot(random_features_train)\n",
    "    return K_train, K_test\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAFcxRzg2tE3"
   },
   "source": [
    "\n",
    "# Dataset loading\n",
    "   ## Here you have three choices (mutag, SBM, DD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_loading:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    #mutag dataset\n",
    "    def mutag(self,test_size=0.1):\n",
    "        Gnx_train=[];\n",
    "        Gnx_test=[];\n",
    "        MUTAG = fetch_dataset(\"MUTAG\", verbose=False,as_graphs=False)\n",
    "        G, y = MUTAG.data, MUTAG.target\n",
    "        G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=0.1, random_state=42)\n",
    "        for i in range(len(G_train)):\n",
    "            g_current=nx.Graph(list(G_train[i][2]));\n",
    "            g_current.add_nodes_from(G_train[i][1])\n",
    "            Gnx_train.append(g_current)\n",
    "        for i in range(len(G_test)):\n",
    "            g_current=nx.Graph(list(G_test[i][2]));\n",
    "            g_current.add_nodes_from(G_test[i][1])\n",
    "            Gnx_test.append(g_current)\n",
    "        return (Gnx_train,y_train), (Gnx_test,y_test)\n",
    "\n",
    "    #SBM generator\n",
    "    def generate_SBM(self,Graphs_num=300,nodes_per_graph=60,block_size=10,fraction=0.3,mult_factor=1.2,avg_deg=10,test_size=0.2):\n",
    "        blocks_num=int(nodes_per_graph/block_size)\n",
    "        sizes=[block_size]*blocks_num\n",
    "        G,y=[],[]\n",
    "        for i in range (Graphs_num):                  \n",
    "            p_in=fraction  if i <Graphs_num/2 else fraction*mult_factor\n",
    "            p_out=(avg_deg-(block_size-1)*p_in)/(nodes_per_graph-block_size)\n",
    "            p=p_out*np.ones([blocks_num]*2)+(p_in-p_out)*np.eye(blocks_num)\n",
    "            #print(p_in,p_out)\n",
    "            G.append(nx.stochastic_block_model(sizes, p, seed=0))\n",
    "            y.append(-1 if i<Graphs_num/2 else 1)            \n",
    "        G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=test_size)\n",
    "        return (G_train,y_train),(G_test,y_test)\n",
    "        \n",
    "\n",
    "    # DD dataset\n",
    "    def DD(self,test_size=0.1,train_size=800):\n",
    "        DD = fetch_dataset(\"DD\", verbose=True)\n",
    "        G, y = DD.data, DD.target\n",
    "        Gnx_train=[];\n",
    "        Gnx_test=[];           # Taking just Train_size graphs of the data set as training set, \n",
    "                                       #this is due to the large computatational time\n",
    "        G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=test_size, random_state=42)\n",
    "        G_train,y_train=G_train[0:Train_size], y_train[0:Train_size]\n",
    "        for i in range(len(G_train)):\n",
    "            g_current=nx.Graph(list(G_train[i][0]));\n",
    "            g_current.add_nodes_from(G_train[i][1])\n",
    "            Gnx_train.append(g_current)\n",
    "        for i in range(len(G_test)):\n",
    "            g_current=nx.Graph(list(G_test[i][0]));\n",
    "            g_current.add_nodes_from(G_test[i][1])\n",
    "            Gnx_test.append(g_current)\n",
    "        return (Gnx_train,y_train), (Gnx_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf_ML_s82tFJ"
   },
   "source": [
    "\n",
    "\n",
    "# The following part includes setting up the hyperparameters and conducting the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the solution: model selection\n",
    "def run_grid(K_train, K_test, y_train, y_test, C_range = 10. ** np.arange(-2, 6)):\n",
    "    param_grid = dict(C=C_range)\n",
    "    grid = GridSearchCV(SVC(kernel='precomputed', gamma='auto'),\n",
    "                        param_grid=param_grid, cv=StratifiedKFold())\n",
    "    print('Fit...')\n",
    "    grid.fit(K_train, y_train)\n",
    "    # Training error\n",
    "    y_pred = grid.predict(K_test)\n",
    "\n",
    "    # Computes and prints the classification accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", str(round(acc*100, 2)) + \"%\")\n",
    "    return acc\n",
    "\n",
    "#run_grid(K_train, K_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6379919,
     "status": "ok",
     "timestamp": 1584722880563,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "kcNEwc63PKPQ",
    "outputId": "e7ed865e-c40d-4957-f07f-9aa323c9bc21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1, 1/2\n",
      "Graph 0/240\n",
      "Graph 10/240\n",
      "Graph 20/240\n",
      "Graph 30/240\n",
      "Graph 40/240\n",
      "Graph 50/240\n",
      "Graph 60/240\n",
      "Graph 70/240\n",
      "Graph 80/240\n",
      "Graph 90/240\n",
      "Graph 100/240\n",
      "Graph 110/240\n",
      "Graph 120/240\n",
      "Graph 130/240\n",
      "Graph 140/240\n",
      "Graph 150/240\n",
      "Graph 160/240\n",
      "Graph 170/240\n",
      "Graph 180/240\n",
      "Graph 190/240\n",
      "Graph 200/240\n",
      "Graph 210/240\n",
      "Graph 220/240\n",
      "Graph 230/240\n",
      "Graph 0/60\n",
      "Graph 10/60\n",
      "Graph 20/60\n",
      "Graph 30/60\n",
      "Graph 40/60\n",
      "Graph 50/60\n",
      "Fit...\n",
      "Accuracy: 95.0%\n",
      "1/1, 2/2\n",
      "Graph 0/240\n",
      "Graph 10/240\n",
      "Graph 20/240\n",
      "Graph 30/240\n",
      "Graph 40/240\n",
      "Graph 50/240\n",
      "Graph 60/240\n",
      "Graph 70/240\n",
      "Graph 80/240\n",
      "Graph 90/240\n",
      "Graph 100/240\n",
      "Graph 110/240\n",
      "Graph 120/240\n",
      "Graph 130/240\n",
      "Graph 140/240\n",
      "Graph 150/240\n",
      "Graph 160/240\n",
      "Graph 170/240\n",
      "Graph 180/240\n",
      "Graph 190/240\n",
      "Graph 200/240\n",
      "Graph 210/240\n",
      "Graph 220/240\n",
      "Graph 230/240\n",
      "Graph 0/60\n",
      "Graph 10/60\n",
      "Graph 20/60\n",
      "Graph 30/60\n",
      "Graph 40/60\n",
      "Graph 50/60\n",
      "Fit...\n",
      "Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "## test many features_num\n",
    "np.random.seed(0)\n",
    "nodes_num, samples_num, p_flyback, sigma= 6, 2000, 0.85, 10\n",
    "mult_factor=1.1 # the closer to 1, the harder the classification problem\n",
    "sampler_type=\"simple_random_sampling\"\n",
    "sampler=graph_sampler(sampler_type, nodes_num)\n",
    "######################################################\n",
    "feat_axis=[10, 25, 50, 100, 250, 500, 1000]\n",
    "num_expe=5\n",
    "result=np.zeros((len(feat_axis), num_expe))\n",
    "dataset=dataset_loading()\n",
    "for (f_ind, features_num) in enumerate(feat_axis):\n",
    "    for i in range(num_expe):\n",
    "        print('{}/{}, {}/{}'.format(f_ind+1, len(feat_axis), i+1, num_expe))\n",
    "        (G_train,y_train),(G_test,y_test) = dataset.generate_SBM(mult_factor=mult_factor, block_size=10) # generate a new synthetic dataset for each expe\n",
    "        #feat_map=Gaussian_random_features(nodes_num**2, features_num, sigma) # generate a new RF for each expe\n",
    "        feat_map=Lighton_random_features(nodes_num**2, features_num) # generate a new RF for each expe\n",
    "        GRF = graphlet_avg_features(samples_num, sampler, feat_map, batch_size=None, verbose=True)\n",
    "        K_train,K_test=calc_kernel(G_train, G_test, GRF)\n",
    "        result[f_ind, i] = run_grid(K_train, K_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1584724061445,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "siMOqwxoWKJN",
    "outputId": "808eb743-bb99-49f8-e1df-461592fe77e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa77dfc58d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANbklEQVR4nO3cf6jdd33H8efLxmx/WKdrLkWSrqmsG70bRd0xmxOXIsMl+6O1cWg7wR/IinPdf/2jxYEQKWWzg1FW5iILpfvDrpRNKipR+oP+o9CT1VZjSI2FrUllva6rUPyjtL73x/2mnF6T3HNzz83Jfft8wIVzvp/POed96Jdnvjknt6kqJEl9vWHeA0iSNpahl6TmDL0kNWfoJak5Qy9JzRl6SWpuy7wHWGnbtm21c+fOeY8hSZvK4cOHf1JVC6dbu+BCv3PnTsbj8bzHkKRNJcl/nWnNj24kqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJam5VUOf5GCS55N8/wzrSXJXkuNJnkryrom1jyf54fDz8VkOLkmazjRX9PcAe86yvhe4cvi5CfgngCS/DnwO+H1gF/C5JG9dz7CSpLVbNfRV9Rjwwlm2XAfcW8u+A7wlyduAPwG+VVUvVNX/Ad/i7H9gSJI2wCw+o98OPDtx/8Rw7EzHf0GSm5KMk4yXlpZmMJIk6ZQL4svYqjpQVaOqGi0sLMx7HElqZRahPwlcNnF/x3DsTMclSefRLEL/IPCx4V/f/AHw06r6MXAI+ECStw5fwn5gOCZJOo+2rLYhyZeBa4BtSU6w/C9p3ghQVV8Evg78KXAc+BnwyWHthSSfBx4fnmp/VZ3tS11J0gZYNfRVdeMq6wX81RnWDgIHz200SdIsXBBfxkqSNo6hl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4ZekpqbKvRJ9iQ5luR4kltPs355koeSPJXk0SQ7Jtb+LsmRJEeT3JUks3wDkqSzWzX0SS4C7gb2AovAjUkWV2y7E7i3qq4G9gN3DI/9Q+C9wNXA7wLvBnbPbHpJ0qqmuaLfBRyvqmeq6mXgPuC6FXsWgYeH249MrBfwq8BW4FeANwL/s96hJUnTmyb024FnJ+6fGI5NehLYN9y+Hrg4ySVV9W2Ww//j4edQVR1d38iSpLWY1ZextwC7kzzB8kczJ4FXk/wmcBWwg+U/HN6f5H0rH5zkpiTjJOOlpaUZjSRJgulCfxK4bOL+juHYa6rquaraV1XvBD47HHuR5av771TVS1X1EvAN4D0rX6CqDlTVqKpGCwsL5/hWJEmnM03oHweuTHJFkq3ADcCDkxuSbEty6rluAw4Ot/+b5Sv9LUneyPLVvh/dSNJ5tGroq+oV4GbgEMuRvr+qjiTZn+TaYds1wLEkTwOXArcPxx8AfgR8j+XP8Z+sqq/O9i1Iks4mVTXvGV5nNBrVeDye9xiStKkkOVxVo9Ot+ZuxktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKamyr0SfYkOZbkeJJbT7N+eZKHkjyV5NEkOybWfiPJN5McTfKDJDtnN74kaTWrhj7JRcDdwF5gEbgxyeKKbXcC91bV1cB+4I6JtXuBL1TVVcAu4PlZDC5Jms40V/S7gONV9UxVvQzcB1y3Ys8i8PBw+5FT68MfCFuq6lsAVfVSVf1sJpNLkqYyTei3A89O3D8xHJv0JLBvuH09cHGSS4DfAl5M8u9JnkjyheFvCJKk82RWX8beAuxO8gSwGzgJvApsAd43rL8beDvwiZUPTnJTknGS8dLS0oxGkiTBdKE/CVw2cX/HcOw1VfVcVe2rqncCnx2Ovcjy1f93h499XgG+Arxr5QtU1YGqGlXVaGFh4RzfiiTpdKYJ/ePAlUmuSLIVuAF4cHJDkm1JTj3XbcDBice+Jcmper8f+MH6x5YkTWvV0A9X4jcDh4CjwP1VdSTJ/iTXDtuuAY4leRq4FLh9eOyrLH9s81CS7wEBvjTzdyFJOqNU1bxneJ3RaFTj8XjeY0jSppLkcFWNTrfmb8ZKUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc1OFPsmeJMeSHE9y62nWL0/yUJKnkjyaZMeK9TcnOZHkH2c1uCRpOquGPslFwN3AXmARuDHJ4optdwL3VtXVwH7gjhXrnwceW/+4kqS1muaKfhdwvKqeqaqXgfuA61bsWQQeHm4/Mrme5PeAS4Fvrn9cSdJaTRP67cCzE/dPDMcmPQnsG25fD1yc5JIkbwD+HrjlbC+Q5KYk4yTjpaWl6SaXJE1lVl/G3gLsTvIEsBs4CbwKfAb4elWdONuDq+pAVY2qarSwsDCjkSRJAFum2HMSuGzi/o7h2Guq6jmGK/okbwI+VFUvJnkP8L4knwHeBGxN8lJV/cIXupKkjTFN6B8HrkxyBcuBvwH488kNSbYBL1TVz4HbgIMAVfXRiT2fAEZGXpLOr1U/uqmqV4CbgUPAUeD+qjqSZH+Sa4dt1wDHkjzN8hevt2/QvJKkNUpVzXuG1xmNRjUej+c9hiRtKkkOV9XodGv+ZqwkNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqztBLUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOamCn2SPUmOJTme5NbTrF+e5KEkTyV5NMmO4fg7knw7yZFh7SOzfgOSpLNbNfRJLgLuBvYCi8CNSRZXbLsTuLeqrgb2A3cMx38GfKyqfgfYA/xDkrfManhJ0uqmuaLfBRyvqmeq6mXgPuC6FXsWgYeH24+cWq+qp6vqh8Pt54DngYVZDC5Jms40od8OPDtx/8RwbNKTwL7h9vXAxUkumdyQZBewFfjRyhdIclOScZLx0tLStLNLkqYwqy9jbwF2J3kC2A2cBF49tZjkbcC/Ap+sqp+vfHBVHaiqUVWNFha84JekWdoyxZ6TwGUT93cMx14zfCyzDyDJm4APVdWLw/03A18DPltV35nF0JKk6U1zRf84cGWSK5JsBW4AHpzckGRbklPPdRtwcDi+FfgPlr+ofWB2Y0uSprVq6KvqFeBm4BBwFLi/qo4k2Z/k2mHbNcCxJE8DlwK3D8c/DPwR8Ikk3x1+3jHrNyFJOrNU1bxneJ3RaFTj8XjeY0jSppLkcFWNTrfmb8ZKUnOGXpKaM/SS1Jyhl6TmDL0kNWfoJak5Qy9JzRl6SWrO0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJas7QS1Jzhl6SmjP0ktScoZek5gy9JDVn6CWpOUMvSc0ZeklqLlU17xleJ8kS8CLw07Ns+7WzrG8DfjLruTbY2d7Phfxa63mutT522v3T7FttT7fzC87fOeb5Nb/z6/KqWjjtSlVdcD/AgXNdB8bznn/W7/dCfa31PNdaHzvt/mn2/bKdX7P+736+Xsfza3Y/F+pHN19d5/pmcz7fzyxfaz3PtdbHTrt/mn2/bOcXnL/35Pl1AZ5fF9xHN+uVZFxVo3nPoZ48v7SRNur8ulCv6NfjwLwHUGueX9pIG3J+tbuilyS9XscreknSBEMvSc0ZeklqrnXok7w9yb8keWDes6inJB9M8qUk/5bkA/OeR70kuSrJF5M8kOQvz/V5Nl3okxxM8nyS7684vifJsSTHk9wKUFXPVNWn5jOpNqs1nmNfqaq/AD4NfGQe82pzWeP5dbSqPg18GHjvub7mpgs9cA+wZ/JAkouAu4G9wCJwY5LF8z+amriHtZ9jfzOsS6u5hzWcX0muBb4GfP1cX3DThb6qHgNeWHF4F3B8uIJ/GbgPuO68D6cW1nKOZdnfAt+oqv8837Nq81lrw6rqwaraC3z0XF9z04X+DLYDz07cPwFsT3JJki8C70xy23xGUxOnPceAvwb+GPizJJ+ex2Bq4UwNuybJXUn+mXVc0W9Z73QXsqr6X5Y/O5U2RFXdBdw17znUU1U9Cjy63ufpckV/Erhs4v6O4Zg0K55j2kgben51Cf3jwJVJrkiyFbgBeHDOM6kXzzFtpA09vzZd6JN8Gfg28NtJTiT5VFW9AtwMHAKOAvdX1ZF5zqnNy3NMG2ke55f/UzNJam7TXdFLktbG0EtSc4Zekpoz9JLUnKGXpOYMvSQ1Z+glqTlDL0nNGXpJau7/AeV7mSwt1ZfoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(feat_axis, np.mean(result,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1584714964368,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "_CYz8Yl0Wuvz",
    "outputId": "3f67dcd2-a118-4353-8bfd-27b153d6b2b0"
   },
   "outputs": [],
   "source": [
    "result\n",
    "np.savetxt('SBM_results_lighton_1.csv', np.array(result), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95      , 0.96666667]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "class_form.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
