{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem - Solution Discription\n",
    "## Problem: testing the discrimination power of GNN GIN-based model on SBM Dataset with different values of inter-classes similarity parameter (mult_factor=1+np.linespace (0.2,2,18)).\n",
    "## Solution: the model is specified in the code within the NET class, with:\n",
    "## batch size=128 , epochs number=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G4gl_HqV-Tgf",
    "outputId": "2b984e68-e19b-436d-ac67-713e46868039"
   },
   "outputs": [],
   "source": [
    "#cu101 \n",
    "!pip install torch-geometric \\\n",
    "  torch-sparse==latest+cu101 \\\n",
    "  torch-scatter==latest+cu101 \\\n",
    "  torch-cluster==latest+cu101 \\\n",
    "  -f https://pytorch-geometric.com/whl/torch-1.4.0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iqmxopj3wL4"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import torch\n",
    "from matplotlib import pyplot as plt \n",
    "#from torch_geometric.datasets import Entities \n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GINConv, global_add_pool, GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import convert\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "80fAb41VNGWi",
    "outputId": "9fecc20c-cc6c-41c1-b1a5-bac5670177e6"
   },
   "outputs": [],
   "source": [
    "class dataset_loading:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    #SBM generator\n",
    "    def generate_SBM(self,Graphs_num=600,nodes_per_graph=60,block_size=10,fraction=0.3,mult_factor=1.5,avg_deg=10,test_size=0.2): #############\n",
    "        blocks_num=int(nodes_per_graph/block_size)\n",
    "        sizes=[block_size]*blocks_num\n",
    "        G,y=[],[]\n",
    "        for i in range (Graphs_num):                  \n",
    "            p_in=fraction  if i <Graphs_num/2 else fraction*mult_factor\n",
    "            p_out=(avg_deg-(block_size-1)*p_in)/(nodes_per_graph-block_size)\n",
    "            p=p_out*np.ones([blocks_num]*2)+(p_in-p_out)*np.eye(blocks_num)\n",
    "            #print(p_in,p_out)\n",
    "            G.append(nx.stochastic_block_model(sizes, p))\n",
    "            y.append(0 if i<Graphs_num/2 else 1)            \n",
    "        G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=test_size)\n",
    "        return (G_train,y_train),(G_test,y_test)\n",
    "\n",
    "# Here is the GNN GIN based model's structure\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        num_features=1    # dimensionality of node features ( in this case we considered node_degree)    \n",
    "        num_classes=2\n",
    "        dim = 32          # dimensionality of hidden layers' outputs \n",
    "\n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)    # batch normalization layer\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    '''\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups: pass\n",
    "            #param_group['lr'] = 0.1 * param_group['lr']\n",
    "    \n",
    "    for param_group in optimizer.param_groups: pass\n",
    "        #param_group['lr'] =  param_group['lr']/(1+0.1*epoch)\n",
    "    '''\n",
    "    # apply decay on learning rate \n",
    "    if epoch%100==0: \n",
    "        for param_group in optimizer.param_groups: \n",
    "            param_group['lr'] = 0.1 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "mult_factor=1+np.linspace(0.2,2,18)             # inter class similarity param \n",
    "train_loss = np.zeros([len(mult_factor),1])\n",
    "train_acc = np.zeros([len(mult_factor),1])\n",
    "test_acc = np.zeros([len(mult_factor),1])\n",
    "for ind in range (len(mult_factor)):\n",
    "    # generating the SBM dataset in Networkx framwork \n",
    "    (Gtr,ytr),(Gts,yts)=dataset_loading().generate_SBM(mult_factor=mult_factor[ind]) \n",
    "    #creating the nodes-degree features vectors \n",
    "    for i,g in enumerate (Gtr):\n",
    "    dg=list(g.degree(g.nodes))\n",
    "    dg=dict([(i,[j]) for (i,j) in dg])\n",
    "    nx.set_node_attributes(g,dg,'x')\n",
    "    #go from networkx to Torch-geormetry framwork \n",
    "    Gtr=[convert.from_networkx(g) for g in Gtr]\n",
    "    for i,g in enumerate (Gtr): g.y=torch.tensor([ytr[i]],dtype=torch.long)\n",
    "\n",
    "    for i,g in enumerate (Gts):\n",
    "    dg=list(g.degree(g.nodes))\n",
    "    dg=dict([(i,[j]) for (i,j) in dg])\n",
    "    nx.set_node_attributes(g,dg,'x')\n",
    "    Gts=[convert.from_networkx(g) for g in Gts]\n",
    "    for i,g in enumerate (Gts): g.y=torch.tensor([yts[i]], dtype=torch.long)\n",
    "    \n",
    "    # setting the cuda setup \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # defining an instance of the model with the optimizer \n",
    "    model = Net().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    \n",
    "    test_dataset = Gts           #redundant        \n",
    "    train_dataset = Gtr          # redundant \n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "    # start the learning and evaluation  processes \n",
    "    print(\"\\nmult factor value is {}\\n\".format(mult_factor[ind]))\n",
    "    for epoch in range(1, 300):\n",
    "        train_loss[ind][0] = train(epoch)\n",
    "        train_acc [ind][0]= test(train_loader)\n",
    "        test_acc [ind][0]= test(test_loader)\n",
    "        #test_acc = 0\n",
    "        print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
    "            'Train Acc: {:.1f}%, Test Acc: {:.0f}%'.format(epoch, train_loss[ind][0],\n",
    "                                                        100*train_acc[ind][0], 100*test_acc[ind][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxdUSsIpmhTy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "W0WFF3WjsWs4",
    "outputId": "71257397-3fff-4a6d-8c30-b96eba60dcf1"
   },
   "outputs": [],
   "source": [
    "# the next three cells are for saving and visualizing the loss-accuracy of the model \n",
    "results =np.concatenate((train_loss,train_acc,test_acc),axis=1)\n",
    "np.savetxt('GNN_model_SBM_variant_similarity.csv',results, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "EDa2fKIIAGcI",
    "outputId": "884ca468-42af-40e7-a3e5-3ddfe555da47"
   },
   "outputs": [],
   "source": [
    "plt.plot(mult_factor,results[:,0].flatten())\n",
    "plt.xlabel(\"Inter-classes similarity parameter \")\n",
    "plt.ylabel(\"Training loss (Cross entropy)\")\n",
    "plt.title(\"GIN-based model performance with different classes similarity\")\n",
    "plt.grid()\n",
    "plt.savefig(\"Model's training loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "5zUNpNynBeiF",
    "outputId": "14383bb6-e56d-4714-b5f7-96f880632365"
   },
   "outputs": [],
   "source": [
    "plt.plot(mult_factor,results[:,1].flatten(),label=\"Training accuracy\",Linewidth=2)\n",
    "plt.plot(mult_factor,results[:,2].flatten(),label=\"Test accuracy\",Linewidth=2)\n",
    "plt.xlabel(\"Inter-classes similarity parameter \")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"GIN-based model performance with different classes similarity\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(\"Model's Accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is unimportant, I was just trying things \n",
    "\n",
    "a=TUDataset(root=\".\", name=\"MUTAG\").shuffle()\n",
    "print(len(a))\n",
    "print(a[0].keys)\n",
    "print(a[0].y)\n",
    "print(a[100].y)\n",
    "a=convert.to_networkx(a[0],node_attrs='x')\n",
    "print()\n",
    "color = nx.get_node_attributes(a, 'x')\n",
    "print(a.nodes[0]['x'])\n",
    "#print(a.nodes(data=True))\n",
    "a=convert.from_networkx(a)\n",
    "print(type(a))\n",
    "print(a.y)\n",
    "a.y=1\n",
    "print(a.y)\n",
    "#nx.draw(a)\n",
    "#plt. show()\n",
    "dataset = TUDataset('.', name='MUTAG').shuffle()\n",
    "test_dataset = dataset[:1]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "\n",
    "print(type(test_dataset))\n",
    "print(100%100==0)\n",
    "print(101%100==0)\n",
    "print(dataset[0].y)\n",
    "print(type(dataset[0]))\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    print(data.x.dtype)\n",
    "print(dataset.num_features)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
