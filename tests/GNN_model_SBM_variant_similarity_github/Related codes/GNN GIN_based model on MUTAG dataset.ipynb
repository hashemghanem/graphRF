{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4gl_HqV-Tgf",
        "colab_type": "code",
        "outputId": "92e4bcae-bc89-4bde-ceb0-4dc7a6680b9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#cu101 \n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse==latest+cu101 \\\n",
        "  torch-scatter==latest+cu101 \\\n",
        "  torch-cluster==latest+cu101 \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-1.4.0.html"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.4.3)\n",
            "Collecting torch-sparse==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting torch-scatter==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting torch-cluster==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.21.0)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.16.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.14.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.8)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (7.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (46.1.3)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (1.2.0)\n",
            "Installing collected packages: torch-sparse, torch-scatter, torch-cluster\n",
            "  Found existing installation: torch-sparse 0.6.1\n",
            "    Uninstalling torch-sparse-0.6.1:\n",
            "      Successfully uninstalled torch-sparse-0.6.1\n",
            "  Found existing installation: torch-scatter 2.0.4\n",
            "    Uninstalling torch-scatter-2.0.4:\n",
            "      Successfully uninstalled torch-scatter-2.0.4\n",
            "  Found existing installation: torch-cluster 1.5.4\n",
            "    Uninstalling torch-cluster-1.5.4:\n",
            "      Successfully uninstalled torch-cluster-1.5.4\n",
            "Successfully installed torch-cluster-1.5.4 torch-scatter-2.0.4 torch-sparse-0.6.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch_cluster",
                  "torch_scatter",
                  "torch_sparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbGjwRFcg4R6",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "bdd19419-0201-452b-83e7-a52464e0e7d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "import torch\n",
        "import networkx as nx \n",
        "from matplotlib import pyplot as plt \n",
        "#from torch_geometric.datasets import Entities \n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "from torch_geometric.utils import convert\n",
        "a=TUDataset(root=\".\", name=\"MUTAG\").shuffle()\n",
        "a=convert.to_networkx(a[0])\n",
        "print(type(a))\n",
        "nx.draw(a)\n",
        "plt. show()\n",
        "\n",
        " "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'networkx.classes.digraph.DiGraph'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJp-VDBhmks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtEvuE7b5xNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        # data = data.to(torch.device('cuda'))\n",
        "        out = model(data.pos, data.batch)\n",
        "        loss = F.nll_loss(F.log_softmax(out, dim=1), data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "for epoch in range(1, 10):\n",
        "    loss = train()\n",
        "    print('Epoch: {:01d}, Loss: {:.4f}'.format(epoch, loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxdUSsIpmhTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dataset = TUDataset('.', name='MUTAG').shuffle()\n",
        "test_dataset = dataset[:len(dataset) // 10]\n",
        "train_dataset = dataset[-4:]\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QxOPZJeVlYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "0bbbb37f-f867-43a1-909e-8f155d94f87f"
      },
      "source": [
        "for data in train_loader:\n",
        "  print(data.batch)\n",
        "print(dataset.num_features)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3])\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2hA-1BrLakG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f859436-1606-4a37-afe9-e512773bc33e"
      },
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import GINConv, global_add_pool, GCNConv, global_mean_pool\n",
        "\n",
        "#path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'MUTAG')\n",
        "dataset = TUDataset('.', name='MUTAG').shuffle()\n",
        "test_dataset = dataset[:len(dataset) // 10]\n",
        "train_dataset = dataset[len(dataset) // 10:]\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        num_features = dataset.num_features\n",
        "        dim = 32\n",
        "        hidden=dim\n",
        "        num_layers=4\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden)                  \n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bn=torch.nn.ModuleList()\n",
        "        for i in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden, hidden))\n",
        "            self.bn.append( torch.nn.BatchNorm1d(hidden) )\n",
        "        self.lin1 = Linear(hidden, hidden)\n",
        "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(dim)                  #######################################################\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        #x, edge_index, batch = data.x, data.edge_index, data.batch          #############################################3\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x=self.bn1(x)\n",
        "        for i,conv in enumerate(self.convs):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x=self.bn[i](x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=0.1, training=self.training)     #######################################################################3\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "\n",
        "    if epoch == 51:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = 0.1 * param_group['lr']\n",
        "\n",
        "    loss_all = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data.x, data.edge_index, data.batch)\n",
        "        loss = F.nll_loss(output, data.y)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data.x, data.edge_index, data.batch)\n",
        "        pred = output.max(dim=1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    train_loss = train(epoch)\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
        "          'Train Acc: {:.7f}, Test Acc: {:.7f}'.format(epoch, train_loss,\n",
        "                                                       train_acc, test_acc))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Train Loss: 0.6967918, Train Acc: 0.6941176, Test Acc: 0.6111111\n",
            "Epoch: 002, Train Loss: 0.5780008, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 003, Train Loss: 0.4958300, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 004, Train Loss: 0.4840325, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 005, Train Loss: 0.4748246, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 006, Train Loss: 0.4502562, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 007, Train Loss: 0.4496816, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 008, Train Loss: 0.4384950, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 009, Train Loss: 0.4018524, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 010, Train Loss: 0.3768635, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 011, Train Loss: 0.3604881, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 012, Train Loss: 0.3766768, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
            "Epoch: 013, Train Loss: 0.4095104, Train Acc: 0.5235294, Test Acc: 0.5555556\n",
            "Epoch: 014, Train Loss: 0.3902161, Train Acc: 0.6647059, Test Acc: 0.6111111\n",
            "Epoch: 015, Train Loss: 0.4036562, Train Acc: 0.3294118, Test Acc: 0.3333333\n",
            "Epoch: 016, Train Loss: 0.3512968, Train Acc: 0.3058824, Test Acc: 0.3888889\n",
            "Epoch: 017, Train Loss: 0.3339569, Train Acc: 0.3000000, Test Acc: 0.3888889\n",
            "Epoch: 018, Train Loss: 0.3340574, Train Acc: 0.3000000, Test Acc: 0.3888889\n",
            "Epoch: 019, Train Loss: 0.3251511, Train Acc: 0.3058824, Test Acc: 0.3888889\n",
            "Epoch: 020, Train Loss: 0.3200515, Train Acc: 0.3058824, Test Acc: 0.3888889\n",
            "Epoch: 021, Train Loss: 0.3071616, Train Acc: 0.3176471, Test Acc: 0.4444444\n",
            "Epoch: 022, Train Loss: 0.3022444, Train Acc: 0.3000000, Test Acc: 0.3888889\n",
            "Epoch: 023, Train Loss: 0.3059500, Train Acc: 0.2941176, Test Acc: 0.3888889\n",
            "Epoch: 024, Train Loss: 0.3017329, Train Acc: 0.3294118, Test Acc: 0.4444444\n",
            "Epoch: 025, Train Loss: 0.2663430, Train Acc: 0.3470588, Test Acc: 0.3888889\n",
            "Epoch: 026, Train Loss: 0.2722303, Train Acc: 0.3294118, Test Acc: 0.3888889\n",
            "Epoch: 027, Train Loss: 0.2616430, Train Acc: 0.3705882, Test Acc: 0.3888889\n",
            "Epoch: 028, Train Loss: 0.2749221, Train Acc: 0.4647059, Test Acc: 0.5000000\n",
            "Epoch: 029, Train Loss: 0.2699605, Train Acc: 0.4529412, Test Acc: 0.5555556\n",
            "Epoch: 030, Train Loss: 0.2805880, Train Acc: 0.4941176, Test Acc: 0.4444444\n",
            "Epoch: 031, Train Loss: 0.2766799, Train Acc: 0.7882353, Test Acc: 0.6666667\n",
            "Epoch: 032, Train Loss: 0.2944831, Train Acc: 0.8000000, Test Acc: 0.6111111\n",
            "Epoch: 033, Train Loss: 0.2712237, Train Acc: 0.8117647, Test Acc: 0.6666667\n",
            "Epoch: 034, Train Loss: 0.2641346, Train Acc: 0.8058824, Test Acc: 0.6111111\n",
            "Epoch: 035, Train Loss: 0.2296307, Train Acc: 0.8058824, Test Acc: 0.6111111\n",
            "Epoch: 036, Train Loss: 0.2619048, Train Acc: 0.8235294, Test Acc: 0.6111111\n",
            "Epoch: 037, Train Loss: 0.2280342, Train Acc: 0.8235294, Test Acc: 0.6666667\n",
            "Epoch: 038, Train Loss: 0.2302206, Train Acc: 0.8470588, Test Acc: 0.6666667\n",
            "Epoch: 039, Train Loss: 0.2286625, Train Acc: 0.8588235, Test Acc: 0.6666667\n",
            "Epoch: 040, Train Loss: 0.2400875, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 041, Train Loss: 0.2162274, Train Acc: 0.8411765, Test Acc: 0.6111111\n",
            "Epoch: 042, Train Loss: 0.2354717, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 043, Train Loss: 0.2291158, Train Acc: 0.6941176, Test Acc: 0.6111111\n",
            "Epoch: 044, Train Loss: 0.2462162, Train Acc: 0.8588235, Test Acc: 0.6666667\n",
            "Epoch: 045, Train Loss: 0.2342426, Train Acc: 0.8352941, Test Acc: 0.5555556\n",
            "Epoch: 046, Train Loss: 0.2683395, Train Acc: 0.8588235, Test Acc: 0.6666667\n",
            "Epoch: 047, Train Loss: 0.2460876, Train Acc: 0.7882353, Test Acc: 0.5555556\n",
            "Epoch: 048, Train Loss: 0.2970845, Train Acc: 0.7705882, Test Acc: 0.5555556\n",
            "Epoch: 049, Train Loss: 0.2683939, Train Acc: 0.7705882, Test Acc: 0.5555556\n",
            "Epoch: 050, Train Loss: 0.2527933, Train Acc: 0.7941176, Test Acc: 0.5555556\n",
            "Epoch: 051, Train Loss: 0.2171748, Train Acc: 0.8058824, Test Acc: 0.6111111\n",
            "Epoch: 052, Train Loss: 0.2155170, Train Acc: 0.8176471, Test Acc: 0.6111111\n",
            "Epoch: 053, Train Loss: 0.2016675, Train Acc: 0.8352941, Test Acc: 0.6666667\n",
            "Epoch: 054, Train Loss: 0.2017488, Train Acc: 0.8470588, Test Acc: 0.6666667\n",
            "Epoch: 055, Train Loss: 0.2063017, Train Acc: 0.8647059, Test Acc: 0.6666667\n",
            "Epoch: 056, Train Loss: 0.1937634, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 057, Train Loss: 0.2114215, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 058, Train Loss: 0.1741801, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 059, Train Loss: 0.2183785, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 060, Train Loss: 0.1902575, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 061, Train Loss: 0.1783975, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 062, Train Loss: 0.1886260, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 063, Train Loss: 0.1868780, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 064, Train Loss: 0.1724315, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 065, Train Loss: 0.1652957, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 066, Train Loss: 0.1803356, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 067, Train Loss: 0.1760936, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 068, Train Loss: 0.1615505, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 069, Train Loss: 0.1770681, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 070, Train Loss: 0.1691348, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 071, Train Loss: 0.1741814, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 072, Train Loss: 0.1796968, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 073, Train Loss: 0.1751511, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 074, Train Loss: 0.1582304, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 075, Train Loss: 0.1653050, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 076, Train Loss: 0.1745698, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 077, Train Loss: 0.1596742, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 078, Train Loss: 0.1806876, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 079, Train Loss: 0.1657782, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 080, Train Loss: 0.1598963, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 081, Train Loss: 0.1603915, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 082, Train Loss: 0.1657634, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 083, Train Loss: 0.1645292, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 084, Train Loss: 0.1594198, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 085, Train Loss: 0.1656077, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 086, Train Loss: 0.1503022, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 087, Train Loss: 0.1577649, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 088, Train Loss: 0.1644065, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 089, Train Loss: 0.1535874, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 090, Train Loss: 0.1648541, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 091, Train Loss: 0.1546925, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 092, Train Loss: 0.1578709, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 093, Train Loss: 0.1540457, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 094, Train Loss: 0.1584141, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 095, Train Loss: 0.1433410, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 096, Train Loss: 0.1560097, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 097, Train Loss: 0.1583672, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 098, Train Loss: 0.1484094, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 099, Train Loss: 0.1546853, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 100, Train Loss: 0.1521991, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 101, Train Loss: 0.1394384, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 102, Train Loss: 0.1380241, Train Acc: 0.8647059, Test Acc: 0.7222222\n",
            "Epoch: 103, Train Loss: 0.1480924, Train Acc: 0.8647059, Test Acc: 0.7222222\n",
            "Epoch: 104, Train Loss: 0.1466601, Train Acc: 0.8647059, Test Acc: 0.7222222\n",
            "Epoch: 105, Train Loss: 0.1489026, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 106, Train Loss: 0.1442320, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 107, Train Loss: 0.1372819, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 108, Train Loss: 0.1338902, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 109, Train Loss: 0.1438752, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 110, Train Loss: 0.1451685, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 111, Train Loss: 0.1338340, Train Acc: 0.8705882, Test Acc: 0.7222222\n",
            "Epoch: 112, Train Loss: 0.1289395, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 113, Train Loss: 0.1242168, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 114, Train Loss: 0.1297325, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 115, Train Loss: 0.1384191, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 116, Train Loss: 0.1301642, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 117, Train Loss: 0.1405051, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 118, Train Loss: 0.1237523, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 119, Train Loss: 0.1296825, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 120, Train Loss: 0.1354003, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 121, Train Loss: 0.1406418, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 122, Train Loss: 0.1185535, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 123, Train Loss: 0.1301172, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 124, Train Loss: 0.1187125, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 125, Train Loss: 0.1336853, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 126, Train Loss: 0.1191100, Train Acc: 0.8941176, Test Acc: 0.7777778\n",
            "Epoch: 127, Train Loss: 0.1251659, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 128, Train Loss: 0.1136025, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 129, Train Loss: 0.1236390, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 130, Train Loss: 0.1162972, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 131, Train Loss: 0.1205293, Train Acc: 0.8941176, Test Acc: 0.7222222\n",
            "Epoch: 132, Train Loss: 0.1148466, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 133, Train Loss: 0.1108353, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 134, Train Loss: 0.1186082, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 135, Train Loss: 0.1110165, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 136, Train Loss: 0.1208142, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 137, Train Loss: 0.1151010, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 138, Train Loss: 0.1159175, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 139, Train Loss: 0.1373898, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 140, Train Loss: 0.1073396, Train Acc: 0.8764706, Test Acc: 0.7222222\n",
            "Epoch: 141, Train Loss: 0.1102060, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 142, Train Loss: 0.1157170, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 143, Train Loss: 0.1171040, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 144, Train Loss: 0.1123707, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 145, Train Loss: 0.1058256, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 146, Train Loss: 0.1027143, Train Acc: 0.8882353, Test Acc: 0.7222222\n",
            "Epoch: 147, Train Loss: 0.1193846, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 148, Train Loss: 0.0996148, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 149, Train Loss: 0.1020097, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 150, Train Loss: 0.1089149, Train Acc: 0.8823529, Test Acc: 0.7222222\n",
            "Epoch: 151, Train Loss: 0.0937392, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 152, Train Loss: 0.1021142, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 153, Train Loss: 0.1038489, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 154, Train Loss: 0.1095372, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 155, Train Loss: 0.0989158, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 156, Train Loss: 0.1002832, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 157, Train Loss: 0.0991085, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 158, Train Loss: 0.1088196, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 159, Train Loss: 0.1034526, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 160, Train Loss: 0.1052855, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 161, Train Loss: 0.0870106, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 162, Train Loss: 0.0886598, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 163, Train Loss: 0.0960194, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 164, Train Loss: 0.0921075, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 165, Train Loss: 0.0948696, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 166, Train Loss: 0.1044708, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 167, Train Loss: 0.0914618, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 168, Train Loss: 0.1073537, Train Acc: 0.8941176, Test Acc: 0.6666667\n",
            "Epoch: 169, Train Loss: 0.0882099, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 170, Train Loss: 0.0874028, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 171, Train Loss: 0.0887885, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 172, Train Loss: 0.0914021, Train Acc: 0.8882353, Test Acc: 0.6666667\n",
            "Epoch: 173, Train Loss: 0.0889642, Train Acc: 0.8941176, Test Acc: 0.6666667\n",
            "Epoch: 174, Train Loss: 0.0957625, Train Acc: 0.8941176, Test Acc: 0.6666667\n",
            "Epoch: 175, Train Loss: 0.0920241, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 176, Train Loss: 0.0843895, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 177, Train Loss: 0.0992727, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 178, Train Loss: 0.0838896, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 179, Train Loss: 0.1140859, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 180, Train Loss: 0.0861317, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 181, Train Loss: 0.0909024, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 182, Train Loss: 0.0889514, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 183, Train Loss: 0.0980926, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 184, Train Loss: 0.0875234, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 185, Train Loss: 0.0857275, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 186, Train Loss: 0.0931782, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 187, Train Loss: 0.0878066, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 188, Train Loss: 0.1057099, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 189, Train Loss: 0.0776348, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 190, Train Loss: 0.0860298, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 191, Train Loss: 0.0891593, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 192, Train Loss: 0.0926991, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 193, Train Loss: 0.0765238, Train Acc: 0.8705882, Test Acc: 0.6666667\n",
            "Epoch: 194, Train Loss: 0.0750709, Train Acc: 0.8764706, Test Acc: 0.6666667\n",
            "Epoch: 195, Train Loss: 0.0783072, Train Acc: 0.9000000, Test Acc: 0.6666667\n",
            "Epoch: 196, Train Loss: 0.0810319, Train Acc: 0.8941176, Test Acc: 0.6666667\n",
            "Epoch: 197, Train Loss: 0.0728314, Train Acc: 0.9058824, Test Acc: 0.6666667\n",
            "Epoch: 198, Train Loss: 0.0856222, Train Acc: 0.8823529, Test Acc: 0.6666667\n",
            "Epoch: 199, Train Loss: 0.0778382, Train Acc: 0.8882353, Test Acc: 0.6666667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}