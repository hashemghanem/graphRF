\addchapheadtotoc
\chapter{Theoretical Analysis}
\label{chapter:analysis}

We recall that the majority of our work (especially the practical side) focus on using Random Features, mainly OPUs' light-speed random features, to approximate the graphlet kernel to approach Graph Classification problems. We also focus in some stage of this work on using Fourier Random Features to approximate the Gaussian kernel. Thus, in this chapter we first introduce the mathematical notions of Graph Kernels and Random Features. Then, we prove the efficiency of replacing the classical Graph Kernels which have two drawbacks (time, memory) by the use of their corresponding Random Features mapping function, this is to be done using concentration inequalities and information preservation concept. Finally, we show the structure of the OPU and its mathematical model. 






\section{Random Features}
Kernel machines are of interest as they approximate any function arbitrarily well with sufficiently large training data set. On the other hand, the methods that operate on the kernel matrix (Gram matrix) require a lot of time in order to compute this matrix and to train the machine; for example, a dataset with half a million training examples might take days to train on modern workstations \citep{rahimi2008random}.
Unlike kernel machines, linear support vector machines and regularized regression run much faster especially with low-dimensionality training data. 
One way to combine the advantages of the linear and nonlinear vector machines is to convert the training and evaluation of any kernel machine into the corresponding operations of a linear machine by mapping data into a relatively low-dimensional randomized feature space.
Instead of considering the implicit lifting function which corresponds to the kernel, it was proposed to explicitly map the data to a low-dimensional Euclidean inner product space using a randomized feature map $z:\mathcal{R}^d \xrightarrow{}\mathcal{R}^D$ so that the inner product between a pair of transformed points approximates their kernel:
\begin{equation}
\label{eq:approx_RF}
k(x,y)=<\phi(x),\phi(y)> \approx z(x)^Tz(y)
\end{equation}
Considering this approximation, we can simply transform the input with $z$ and then apply a fast linear learning method to approximate the answer of the real kernel. \newline
In what follows, Random Fourier Features method to construct the random feature map function $z$ is presented.

\subsection{Random Fourier Features}
The following Theorem represents the key idea behind this mapping technique
%\newtheorem{theorem}{Theorem}
\begin{theorem}[Bochner's theorem]
A continuous and shift-invariant kernel $k(x,y)=k(x-y$ on $\mathcal{R}^d$ is positive definite if and only if $k(\delta)$ is the Fourier transform of a non-negative measure.
\end{theorem}
What that means is that when a shift-invariant kernel $k$ is properly scaled, its Fourier transform $p(w)$ is a proper probability distribution, we can write:
\begin{equation}
\label{Fourier integral}
k(x-y)=\int_{\mathcal{R}^d}p(w)e^{j{w}'(x-y)}dw=E_w[e^{j{w}'x}{e^{j{w}'y}}^*]
\end{equation}
But both $p(w)$ and $k(\delta)$ are real-valued functions, thus from Eq.~\ref{Fourier integral} we can prove that:
\begin{equation}

k(x-y)=\int_{\mathcal{R}^d}p(w)cos({{w}'(x-y)})dw=E_w[z_w(x)z_w(y)]
\end{equation}
where $z_w(x)=\sqrt{2}cos({w}'x+b)$ such that $w$ is drawn from $p(w)$ and b is drawn uniformly from $[0,2\pi]$.\newline
As a straight result, $\ z_w(x)z_w(y)$ is an unbiased estimate of k(x,y). We can achieve lower variance estimation to the expectation (Eq. \ref{real Fourier integral}) by averaging $D$ instances of the estimator with different random frequencies $w$. i.e. the low-variance estimator can be written as: $z(x)'z(y)=\frac{1}{D} \Sigma_{j=1}^D z_{w_j}(x)z_{w_j}(y)$. this estimator and based on Hoeffding's inequality guarantees exponentially fast convergence in $D$ between $z(x)'z(y)$ and the kernel true value:
\begin{equation}
    Pr(|z(x)'z(y)-k(x,y)|\geq\epsilon)\leq2e^\frac{-D\epsilon^2}{4}
\end{equation}

\subsection{Random features from compressed sensing's  point of view}
Random features can be tackled within a different paradigm in signal processing, in compressed sensing for instance a random projection of a high-dimensional but sparse or compressible signal onto a lower-dimensional space has been shown to contain enough information to be used to reconstruct the original signal with small error margins. In this domain, the well-known Johnson-Lindernstrauss (JL) lemma states that with high probability the geometry of a point cloud is preserved by certain Lipschitz mapping onto a space of dimension logarithmic in the number of points, and with the help of concentration inequalities for random inner products more efficient algorithms for constructing such embeddings were developed. First, we recall $\ell_p^N$ norm of a vector $x\in\mathcal{R}^N$:
\begin{equation}
    \left \| x\right\|_{\ell_p^N}=
    \left\{\begin{matrix}
{(\Sigma_{i=1}^N x_i^p)}^\frac{1}{p}\qquad,  0<p<\infty\\ 
max_{i=1,...,N}\left \| x_i\right\|~~\quad, p=\infty
\end{matrix}\right.
\end{equation}
In the discrete compressed sensing, we want to economically record information about a vector $x\in \mathcal{R}^N$, thus we allocate a group of n nonadaptive questions to ask about $x$. Each question takes the form of a linear function applied to $x$, i.e. the extracted information can be written in the form: 
\begin{equation}
    y=\phi x
\end{equation}
where $\phi \in \mathcal{R}^{n\times N}$ and $n$ is much smaller than $N$. \newline
To reconstruct the original signal from the information that $y$ holds about $x$, a decoder $\Delta$ is used to provide an approximation $\bar{x}=\Delta(y)=\Delta(\phi x)$. It should be noticed in our case with graph classification problem using random features that it is not one of our concerns to find or prove the proficiency of any decoder, but it is important to cover the mathematics that prove the efficiency of random projections represented in compressed sensing by the pair $(\phi, \Delta)$ . to measure the performance of an encoder-decoder pair $(\phi, \Delta)$, we use a norm $\left\| .\right\|_X$ to quantify the error: 
\begin{equation}
    e(x,\phi,\Delta)_X=\left\| x-\Delta(\phi x)\right\|_X
\end{equation}
which is the error of the pair $(\phi, \Delta)$ on $x$. Moreover, if K is any compact set in $\mathcal{R}^N$, then the error of this pair on K is:
\begin{equation}
    e(K,\phi,\Delta)_X = \underset{x\in K}{sup} ~e(x,\phi,\Delta)_X
\end{equation}
To find the best pair that minimizes the previous error, we introduce the set $\mathcal{A}_{n,N}=\{(\phi, \Delta ): \phi \in \mathcal{R}^{n\times N}\}$, thus the best performance of such pair on $K$ is given by: 
\begin{equation}
e_{n,N}(K)_X= \underset{(\phi, \Delta)\in \mathcal{A}_{n,N}}{inf}~e(K,\phi,\Delta)_X
\end{equation}
However, it was proven that if $K\subset \mathcal{R}^N$ such that $K=-K$ and that $K+K\subset C_1K$ ,where $C_1$ is a constant, then\citep{concentration_kashin}:
\begin{equation}
\label{3.5}
   d^n(K)_X < e_{n,N}(K)_X<C_1d^n(K)_X
\end{equation}
where $d^n(K)_X$ is Gelfand width of $K$ in the Banach space $X$:
\begin{equation}
    d^n(K)_X= \underset{codim(Y)=n}{inf}~\underset{x\in K\cap Y}{sup}\left\|x\right\|_X
\end{equation}
Here the best spaces $Y$ are those that slice through $K$ in the most economical direction to minimize the diameter of the set $K\cap Y$ (which corresponds to the direction with minimum variance notion used in Principle Component Analysis PCA). An important result of Gelfand widths that can be used in our paradigm is the limits of this width on the unit ball $K=U(\ell_1^N)$, it states that there exists a constant $C_0>0$ such that the following condition is satisfied whenever $0< n< N $:
\begin{equation}
    C_0^{-1}\sqrt{\frac{log(N/n)+1}{n}}\leq d^n(U(\ell_1^N))_{\ell_2^N}\leq C_0\sqrt{\frac{log(N/n)+1}{n}}
\end{equation}
Back to the proof of Eq.~\ref{3.5} Which can be done by checking the correspondence between $Y$ and $\phi$, where given any matrix $\phi$, its null space $\mathcal{N}$ is a valid candidate for $Y$ in computing $d^n(K)_X$. On the other hand, given any $Y$ of co-dimension $n$ used in computing $d^n(K)_X$, any basis for the orthogonal complement of $Y$ can be used as the rows of a matrix $\phi$ to estimate $E_{n,N}(K)_X$. For example, the unit ball $U(\ell_1^N)$ in $\ell_2^N$ satisfies that $U(\ell_1^N)+U(\ell_1^N)\subset 2U(\ell_1^N)$, thus for all $0< n< N$ : 
\begin{equation}
\label{3.6}
    C_0^{-1}\sqrt{\frac{log(N/n)+1}{n}}\leq E_{n,N}(U(\ell_1^N))_{\ell_2^N}\leq 2C_0\sqrt{\frac{log(N/n)+1}{n}}
\end{equation}
The main problem then is to find the pair $(\phi, \Delta)$ which provides estimates like Eq.~ \ref{3.6}, to address this question an isometry condition on $\phi$ was introduced. Given a matrix $\phi$ and any set T of column indices, we refer by $\phi_T$ to the $n\times \#(T)$ matrix composed of these columns. Simultaneously, for every $x\in \mathcal{R}^N$, we refer by $x_T$ to the vector obtained by considering only the entries in $x$ which correspond to the column indices $T$. We say that a matrix $\phi$ satisfies the RIP (Ristricted Isometry Property) of order k if there exists a $\delta_k\in [0,1]$ such that:
\begin{equation}
    \label{3.8}
    (1-\delta_k)\left\|x_T\right\|_{\ell_2^N}^2\leq
    \left\|\phi_Tx_T\right\|_{\ell_2^N}^2\leq  (1+\delta_k)\left\|x_T\right\|_{\ell_2^N}^2
\end{equation}
holds for every such set $T$ with $\#T\leq k$. The good matrices $\phi$ should satisfy the RIP condition for the largest possible $k$. For instance, if $\phi$ satisfies the RIP of order $3k$ with $\delta_{3k}<1$, then:
\begin{equation}
\label{3.9}
\left\|x-\Delta(\phi x)\right\|\leq \frac{c_2\sigma_k(x)_{\ell_1^N}}{\sqrt(k)}
\end{equation}
where $\sigma_k(x)_{\ell_1^N}$ is the $\ell_1$ error of the best k-term approximation and the constant $C_2$ depends only on $\delta_{3k}$. Since $\sigma_k(x)_{\ell_1^N}\leq \left\|x\right\|_{\ell_1^N}$ , we can obtain the best possible performance correspondent to Eq.\ref{3.6} if we can find a matrix $\phi$ that meet the RIP condition for $k$ of order $n/(log(N/n)+1)$.\newline
Now the question is how to construct such matrix $\phi$ that satisfy the RIP for the largest possible range of $k$. Actually, the only known constructions yielding such matrices are based on random matrices \citep{concentration_kashin}. It is shown that matrices built using random entries drawn from certain probability distributions will meet the RIP Condition with high probability. More specifically, for these constructions of $\phi$, the RIP follows in a simple way from the same concentration of measure inequalities for inner products that have employed to prove the JL lemma (which will not be stated here being irrelevant). \newline
Briefly, it is shown \citep{concentration_achlioptas} that any random variable which satisfies certain moment conditions, the matrix $\phi$ whose entries are independent realizations of this variable can be proven to satisfy the following concentration inequality for any $\epsilon \in [0,1]$:
\begin{equation}
\label{4.3}
    Pr(|~ \left\| \phi x\right\|_{\ell_2^N}^2- \left\|x \right\|_{\ell_2^N}^2~|\geq \epsilon\left\| x\right\|_{\ell_2^N}^2 )\leq 2e^{-nc_0(\epsilon)}
\end{equation}
Where $c_0(\epsilon)$ is only a function of $\epsilon$. The point now is to prove satisfying Eq.\ref{4.3} with some covering arguments is sufficient to prove the RIP for the corresponding matrix $\phi$, then some examples of these distributions are presented.
Considering the same aforementioned notation $\phi_T, X_T$ with $\#T\leq k$ with $\ell_2$ norm, the proof includes constructing  nets of points in each k-dimensional subspace, apply \ref{4.3} to all of these points through a union bound, and then extend the result from our finite set of points to all possible k-dimensional signals. 
\newtheorem{lemma}{Lemma} 
\begin{lemma}
let $\phi(w), w\in \omega^{n\times N}$, be  a random matrix of size $n\times N$ drawn from any distribution that satisfies the concentration inequality (\ref{4.3}). Then, for any set T with $\#T= k<n$ and any $0< \delta< 1$, we have
\begin{equation}
(1-\delta) \| x\|_{\ell_2^N}  
\leq
\| \phi(w)x\|_{\ell_2^N} 
\leq
(1_+\delta) \| x\|_{\ell_2^N} ~for ~all ~x\in X_T
\label{5.1}
\end{equation}
with probability
\begin{equation}
\geq 1-2(12/\delta)^k e^{-c_0(\delta/2)n}
\label{5.2}
\end{equation}

\end{lemma}

The first step of the proof is noticing that since $\phi$ represents a linear transformation, it is enough to prove \ref{5.1} in the case $\|x\|_{\ell_2^N}=1$. The next step is to choose a finite set of points $Q_T\subset X_T$ with $\|q\|_{\ell_2^N}=1$ for all $q\in Q_T$, that satisfies:
\begin{equation}
    \underset{q\in Q_T}{min}~ \|x-q\|_{\ell_2^N}\leq\delta/4 ~~for ~all ~x\in X_T
\end{equation}
The existence of such group is proven within the scope of covering numbers where also we can find that $\#Q_T\leq (12/\delta)^k$. Now, using the union bound to apply \ref{4.3} to this $Q_T$ with $\epsilon=\delta/2$, with
probability exceeding the right side of \ref{5.2}, we have:
\begin{equation}
(1-\delta/2) \| q\|_{\ell_2^N}  
\leq
\| \phi(w)q\|_{\ell_2^N} 
\leq
(1+\delta/2) \| q\|_{\ell_2^N} ~for ~all ~q\in Q_T
\end{equation}
Let A be the smallest number such that:
\begin{equation}
     \| \phi(w)x\|_{\ell_2^N} \leq (1+A) \| x\|_{\ell_2^N} ~for ~all ~x\in X_T
\label{5.6}
\end{equation}
But there is $q\in Q_T$ such that $\| x-q\|_{\ell_2^N}\leq \delta/4 $, so we have:
\begin{equation}
    \| \phi(w)x\|_{\ell_2^N} \leq
    \| \phi(w)q\|_{\ell_2^N} +
    \| \phi(w)(x-q)\|_{\ell_2^N} \leq
    1+\delta/2 +(1+A)\delta/4
    \label{5.7}
\end{equation}
By a simple comparison between Eq.\ref{5.6} and Eq. \ref{5.7}, we get that $A\leq \delta$, which is what we seek proving the upper inequality in Eq. \ref{5.1}, and the lower one can be proven in a similar way.
\begin{theorem}
when n<N, and $0< \delta< 1$,  If the probability distribution generating the $n\times N$ matrices $\phi(w), w\in \Omega^{n\times N}$, satisfies the concentration inequality \ref{4.3}, then there exist constants $c_1, c_2> 0$ depending only on $\delta$ such that
the RIP condition holds for $\phi(w)$ with the prescribed $\delta$ and any $k \leq c_1n/log(N/k)$ with probability $\leq 1-2e^{-c_2n}$.
\end{theorem}
To prove this theorem, from Eq.\ref{5.1} we know that the matrix $\phi(w)$ doesn't satisfy the inequality with probability $\leq 2(12/\delta)^ke^{-c_0(\delta/2)n}$ for each of the k-dimensional sub-spaces. But there are $\binom{N}{k}\leq (eN/k)^k$ such sub-spaces. Thus, this probability for all the sub-spaces becomes: 
\begin{equation}
    \leq  2(eN/k)^k(12/\delta)^ke^{-c_0(\delta/2)n}=2e^{-c_0(\delta/2)n+k[log(12/\delta)+log((eN/k))]}
\label{5.10}
\end{equation}{}
So for a fixed $c_1>0$, and $k\leq c_1n/log(N/k) $, we have that the exponent in Eq. \ref{5.10} is $\leq -c_2n$ provided that $c_2\leq c_0(\delta/2)-c_1[1+(1+log(12/\delta))/log(N/k)]$. Hence, when $c_1>0$ is sufficiently small we have $c_2>0$. So this is what we seek, This proves that with probability $1-2e^{-c_2n}$, the matrix $\phi(w)$ will satisfy Eq. \ref{5.1} on all the k-dimensional sub-spaces for the range of $k\leq c_1' n/[log(N/n) + 1]$ for $c_1'$ depending only on the aforementioned $c_1$.\newline 
The main example of such distributions that satisfy the inequality in \eqref{4.3} (so the moments of it satisfy the prerequisites of this inequality)  is the random matrix $\phi$ whose entries are independent realizations of Gaussian random variable \citep{concentration_kashin}:
\begin{equation}
    \phi_{i,j}\sim \mathcal{N}(0,\frac{1}{n})
\end{equation}
Where the corresponding constant $c_0$ is $c_0(\epsilon)=\epsilon^2/4-\epsilon^3/6$.
Another two examples related to Bernouli distribution which has the same value of the constant $c_0(\epsilon)$:
\begin{equation}
    \phi_{i,j}= 
    \left\{\begin{matrix}
    +1/\sqrt{n} ~ with ~probability ~\frac{1}{2}
\\ 
-1/\sqrt{n} ~ with ~probability ~\frac{1}{2}
\end{matrix}\right.
\end{equation}

\begin{equation}
    \phi_{i,j}= 
    \left\{\begin{matrix}
    +\sqrt{3/n} ~ with ~probability ~\frac{1}{6}
\\ 
0 ~ with ~probability ~\frac{2}{3}
\\
 -\sqrt{3/n} ~ with ~probability ~\frac{1}{6}

\end{matrix}\right.
\end{equation}


\subsection{Mean kernel and random features} \label{subsec:MMD}
%\paragraph{Mean kernel and MMD} 
The mean kernel methodology allows to \emph{lift} a kernel from a domain $\mathcal{X}$ to a kernel on \emph{probability distributions} on $\mathcal{X}$. Given a base kernel $k$ and two probability distribution $P,Q$, it is defined as
\begin{equation}\label{eq:mean_kernel}
k(P,Q) = \mathbb{E}_{x \sim P, y \sim Q} k(x,y)
\end{equation}
In other words, the mean kernel is just the expectation of the base kernel with respect to each term. The associated Euclidean metric is referred to by the  \emph{Maximum Mean Discrepancy (MMD)}, and is naturally defined as:
\begin{equation}\label{eq:MMD}
MMD(P,Q) = \sqrt{k(P,P) + k(Q,Q) - 2k(P,Q)}
\end{equation}
It should be noticed here that $k(P,P) = \mathbb{E}_{x \sim P, x' \sim P} k(x,x') \neq \mathbb{E}_{x \sim P} k(x,x)$.
NOw given data points $(x_1, \ldots, x_n)$ drawn $iid$ from $P$ and $(y_1, \ldots, y_n)$ drawn $iid$ from $Q$, the kernel in Eq. \ref{eq:mean_kernel} can naturally be approximated by:
\begin{equation}\label{eq:mean_kernel_approx}
k(P,Q) \approx \frac{1}{n^2} \sum_{i,j=1}^n k(x_i,y_j)
\end{equation}
and the corresponding approximate MMD is (other variants exist):
\[
MMD(P,Q) \approx \sqrt{\frac{1}{n^2} \sum_{i,j=1}^n k(x_i,x_j) + k(y_i,y_j) - k(x_i,y_j) - k(x_j, y_i)}
\]

\paragraph{MMD and random features}
Mean kernel works especially well with random features. Combining \eqref{eq:approx_RF} and \eqref{eq:mean_kernel_approx}, it is not hard to see that using random features the mean kernel can be further approximated by:
\begin{equation}
\label{eq:mean_kernel_RF}
k(P,Q) \approx \frac{1}{n^2} \sum_{i,j=1}^n \phi(x_i)^*\phi(y_j) = \left(\frac{1}{n} \sum_i \phi(x_i)\right)^* \left(\frac{1}{n} \sum_i \phi(y_i)\right)
\end{equation}
So the computation can be drastically improved by first computing the \emph{averaged random features} (also called random \emph{generalized moments}, also called \emph{sketch}) $\frac{1}{n} \sum_i \phi(x_i)$, and taking a linear kernel between them. The corresponding MMD is then just the Euclidean metric between the averaged random features
\[
MMD(P,Q) \approx \| \frac{1}{n} \sum_i \phi(x_i) - \frac{1}{n} \sum_i \phi(y_i)\|_2
\]

\paragraph{MMD for discrete distributions}
for a discrete space of objects $H_1, \ldots, H_N$ with discrete probability distributions $P = [P_1, \ldots, P_N]$ and $Q$ on them, the mean kernel \ref{eq:mean_kernel} takes a particular form:
\[
k(P,Q) = \sum_{i,j=1}^N P_i Q_j k(H_i, H_j)
\]

\paragraph{MMD with random features on discrete distrbutions}

To combine both notions One can see the link with graphlet sampling, where $f_G$ is the (discrete) probability distribution of the graphlets. If we define $k(F, F') \approx \phi(F)^*\phi(F')$ where $\phi$ is a random feature map that replaces $\phi_k$, then the feature map \eqref{eq:graphlet_kernel_approx} is exactly what appears in \eqref{eq:mean_kernel_RF}. So, now, all the game becomes to find a good feature map $\phi(F)$ for graphlets. The induced MMD metric between graphs is the MMD between graphlets probability distributions $f_G$:
\[
d(G,G') = MMD(f_G, f_{G'}) = \sqrt{k(f_G, f_{G}) + k(f_{G'}, f_{G'}) - 2 k(f_G, f_{G'})} \approx \| \frac{1}{n} \sum_i \phi(F_i) - \frac{1}{n} \sum_i \phi(F'_i)\|_2
\]
where $F_i$ are graphlets drawn from $G$ and $F'_i$ are graphlets drawn from $G'$.



\section{complexity analysis}

Kernel machines are of interest as they approximate any function arbitrarily well with sufficiently large training data set. On the other hand, the methods that operate on the kernel matrix (Gram matrix) require a lot of time in order to compute this matrix and to train the machine; for example, a dataset with half a million training examples might take days to train on modern workstations \citep{rahimi2008random}.
Unlike kernel machines, linear support vector machines and regularized regression run much faster especially with low-dimensionality training data. 
One way to combine the advantages of the linear and nonlinear vector machines is to convert the training and evaluation of any kernel machine into the corresponding operations of a linear machine by mapping data into a relatively low-dimensional randomized feature space.
Instead of considering the implicit lifting function which corresponds to the kernel, it was proposed to explicitly map the data to a low-dimensional Euclidean inner product space using a randomized feature map $z:\mathcal{R}^d \xrightarrow{}\mathcal{R}^D$ so that the inner product between a pair of transformed points approximates their kernel:
\begin{equation}
%\label{eq:approx_RF}
k(x,y)=<\phi(x),\phi(y)> \approx z(x)^Tz(y)
\end{equation}
Considering this approximation, we can simply transform the input with $z$ and then apply a fast linear learning method to approximate the answer of the real kernel. \newline
In what follows, Random Fourier Features method to construct the random feature map function $z$ is presented.



