{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omPBbM1m2tEa"
   },
   "source": [
    "# Gaussian kernel Approximation\n",
    "### In this code we approximate the Gaussian kernel of two graphs by the dot product of two k-random features consinus vectors, each belongs to the corresponding graph\n",
    "### Datasets used: mutag, SBM or DD dataset\n",
    "### Graph sampling technique used in this code: Simple Random Sampling, Random Walk With flyback, simple random walk sampling\n",
    "### Machine learning algo: SVM with kernel\n",
    "\n",
    "## This code can be uploaded on colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1614,
     "status": "ok",
     "timestamp": 1584695839638,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "ATWEei5-2tEe",
    "outputId": "eaa3b7ba-d995-4e9d-fcd2-6db4a029a462"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nWe can follow Random features for large-scale kernel machines, A. Rahimi and B. Recht (2007) and \\nsolve a standard ridge regression on a nonlinear mapping of the data to a new feature space of a \\ndifferent dimension.\\n\\nWhen the number of random projections m tends to infinity, the inner product between the projected\\ndata points approximates a kernel function, due to the concentration of measure (Computation with \\ninfinite neural networks, C. Williams, 1998).\\n\\nThe matrix-vector multiplication outputs a (1×m) vector, complex-valued. This is followed by the \\nelement-wise non-linearity |.|2 and the quantization due to analog to digital conversion. Finally,\\nthe output of the OPU is y a column vector of size (1×m) of type uint8. The independence of the \\nentries of the output vector means that the rows of the matrix R are independent.\\n\\nThe OPU requires a binary matrix of type uint8 as input.\\n\\nyou need: 1. encoder\\n          2. opu (random mapping)\\n          3. decoder\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import random\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings                                     # from here on, it is from lighton\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "from lightonml.projections.sklearn import OPUMap\n",
    "from lightonml.encoding.base import NoDecoding\n",
    "from lightonml.encoding.base import NoEncoding\n",
    "\n",
    "'''\n",
    "We can follow Random features for large-scale kernel machines, A. Rahimi and B. Recht (2007) and \n",
    "solve a standard ridge regression on a nonlinear mapping of the data to a new feature space of a \n",
    "different dimension.\n",
    "\n",
    "When the number of random projections m tends to infinity, the inner product between the projected\n",
    "data points approximates a kernel function, due to the concentration of measure (Computation with \n",
    "infinite neural networks, C. Williams, 1998).\n",
    "\n",
    "The matrix-vector multiplication outputs a (1×m) vector, complex-valued. This is followed by the \n",
    "element-wise non-linearity |.|2 and the quantization due to analog to digital conversion. Finally,\n",
    "the output of the OPU is y a column vector of size (1×m) of type uint8. The independence of the \n",
    "entries of the output vector means that the rows of the matrix R are independent.\n",
    "\n",
    "The OPU requires a binary matrix of type uint8 as input.\n",
    "\n",
    "you need: 1. encoder\n",
    "          2. opu (random mapping)\n",
    "          3. decoder\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings                                     # from here on, it is from lighton\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "from lightonml.projections.sklearn import OPUMap\n",
    "from lightonml.encoding.base import NoDecoding\n",
    "from lightonml.encoding.base import NoEncoding\n",
    "import numpy as np\n",
    "a=np.uint8(np.random.randint(0,2,[5,5]))\n",
    "random_mapping = OPUMap(n_components=5)\n",
    "train_random_features = random_mapping.transform(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://lighton-pypi:****@nexus.lighton.ai/repository/lighton/simple\n",
      "Requirement already satisfied: grakel in /home/ghanemh/.local/lib/python3.7/site-packages (0.1b7)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from grakel) (1.17.5)\n",
      "Requirement already satisfied: nose>=1.1.2 in /home/ghanemh/.local/lib/python3.7/site-packages (from grakel) (1.3.7)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.7/dist-packages (from grakel) (0.22.2)\n",
      "Requirement already satisfied: cython>=0.27.3 in /home/ghanemh/.local/lib/python3.7/site-packages (from grakel) (0.29.16)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/ghanemh/.local/lib/python3.7/site-packages (from grakel) (1.14.0)\n",
      "Requirement already satisfied: future>=0.16.0 in /home/ghanemh/.local/lib/python3.7/site-packages (from grakel) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19->grakel) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19->grakel) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install grakel\n",
    "import grakel as gk\n",
    "from grakel.utils import graph_from_networkx\n",
    "from grakel.datasets import fetch_dataset\n",
    "from grakel import Graph\n",
    "from grakel.kernels import ShortestPath\n",
    "from grakel.kernels import GraphletSampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jvh7-0e2tEp"
   },
   "source": [
    "## The next two blocks are the graph_sampling class and the kernel_class \n",
    "### There is no code other than the classes' methods/attributes, so it is recommended that after compiling these two blocks, you go directly to next blocks and then go back when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0_YN-kj2tEq"
   },
   "outputs": [],
   "source": [
    "class graph_sampler:\n",
    "    '''\n",
    "    \n",
    "    This class provides four sampling techniques:\n",
    "    1. simple_random_sampling\n",
    "    2. simple_random_walk_sampling\n",
    "    3. random_walk_flyback_sampling\n",
    "    4. random_walk_induced_graph_sampling\n",
    "    \n",
    "    Note that when initializing a new instance of this class, sampler_type should be the name of the\n",
    "    required technique as specified above, except for simple_random_walk_sampling where sampler_type\n",
    "    must be a tuple (\"random_walk_flyback_sampling\", p_flyback)\n",
    "\n",
    "    In case one wants to preprocess graphlets on-the-fly, a preprocessing function Adj->vector should be provided.\n",
    "    \n",
    "    After you initialize an instance, you can sample your Graph/list of Graphs by calling sample \n",
    "    method ( the last method)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,sampler_type,nodes_num,preprocess=None):\n",
    "        if preprocess is None:\n",
    "            preprocess=lambda x:x.flatten()\n",
    "        self.preprocess=preprocess\n",
    "        self.nodes_num=nodes_num\n",
    "        if(type(sampler_type)==tuple): # ex: this is the case of random_walk_flyback (name, p_flyback)\n",
    "            self.sampler_type=sampler_type[0]\n",
    "            if(sampler_type[0]==\"random_walk_flyback_sampling\"):self.p_flyback=sampler_type[1]\n",
    "        else : self.sampler_type=sampler_type\n",
    "    \n",
    "    def simple_sampling(self,G,nodes_num):     # one simple_random_sample of G\n",
    "        return G.subgraph(random.sample(G.nodes(), nodes_num))\n",
    "\n",
    "    def random_walk_sampling_simple(self,complete_graph, nodes_to_sample):  # also just one sample using this method\n",
    "        T,growth_size=100,2  # number of iterations (attempts to sample the graph)\n",
    "        complete_graph = nx.convert_node_labels_to_integers(complete_graph, 0, 'default', True)\n",
    "        # giving unique id to every node same as built-in function id\n",
    "        for n, data in complete_graph.nodes(data=True):\n",
    "            complete_graph.node[n]['id'] = n\n",
    "\n",
    "        nr_nodes = len(complete_graph.nodes())\n",
    "        upper_bound_nr_nodes_to_sample = nodes_to_sample\n",
    "        index_of_first_random_node = random.randint(0, nr_nodes-1)\n",
    "        sampled_graph = nx.Graph()\n",
    "\n",
    "        sampled_graph.add_node(complete_graph.node[index_of_first_random_node]['id'])\n",
    "\n",
    "        iteration = 1\n",
    "        edges_before_t_iter = 0\n",
    "        curr_node = index_of_first_random_node\n",
    "        while sampled_graph.number_of_nodes() != upper_bound_nr_nodes_to_sample:\n",
    "            edges = [n for n in complete_graph.neighbors(curr_node)]\n",
    "            index_of_edge = random.randint(0, len(edges) - 1)\n",
    "            chosen_node = edges[index_of_edge]\n",
    "            sampled_graph.add_node(chosen_node)\n",
    "            sampled_graph.add_edge(curr_node, chosen_node)\n",
    "            curr_node = chosen_node\n",
    "            iteration = iteration+1\n",
    "\n",
    "            if iteration % T == 0:\n",
    "                if ((sampled_graph.number_of_edges() - edges_before_t_iter) < growth_size):\n",
    "                    curr_node = random.randint(0, nr_nodes-1)\n",
    "                edges_before_t_iter = sampled_graph.number_of_edges()\n",
    "        return sampled_graph\n",
    "    \n",
    "    def random_walk_sampling_with_fly_back(self,complete_graph, nodes_to_sample, fly_back_prob): # returns one sample\n",
    "        growth_size,T=2,100       # number of iterations (attempts to sample the graph)\n",
    "        complete_graph = nx.convert_node_labels_to_integers(complete_graph, 0, 'default', True)\n",
    "        # giving unique id to every node same as built-in function id\n",
    "        for n, data in complete_graph.nodes(data=True):\n",
    "            complete_graph.node[n]['id'] = n\n",
    "\n",
    "        nr_nodes = len(complete_graph.nodes())\n",
    "        upper_bound_nr_nodes_to_sample = nodes_to_sample\n",
    "\n",
    "        index_of_first_random_node = random.randint(0, nr_nodes-1)\n",
    "        sampled_graph = nx.Graph()\n",
    "\n",
    "        sampled_graph.add_node(complete_graph.node[index_of_first_random_node]['id'])\n",
    "\n",
    "        iteration = 1\n",
    "        edges_before_t_iter = 0\n",
    "        curr_node = index_of_first_random_node\n",
    "        while sampled_graph.number_of_nodes() != upper_bound_nr_nodes_to_sample:\n",
    "            edges = [n for n in complete_graph.neighbors(curr_node)]\n",
    "            index_of_edge = random.randint(0, len(edges) - 1)\n",
    "            chosen_node = edges[index_of_edge]\n",
    "            sampled_graph.add_node(chosen_node)\n",
    "            sampled_graph.add_edge(curr_node, chosen_node)\n",
    "            choice = np.random.choice(['prev','neigh'], 1, p=[fly_back_prob,1-fly_back_prob])\n",
    "            if choice == 'neigh':\n",
    "                curr_node = chosen_node\n",
    "            iteration=iteration+1\n",
    "\n",
    "            if iteration % T == 0:\n",
    "                if ((sampled_graph.number_of_edges() - edges_before_t_iter) < growth_size):\n",
    "                    curr_node = random.randint(0, nr_nodes-1)\n",
    "                    print (\"Choosing another random node to continue random walk \")\n",
    "                edges_before_t_iter = sampled_graph.number_of_edges()\n",
    "\n",
    "        return sampled_graph\n",
    "    \n",
    "    \n",
    "    def random_walk_induced_graph_sampling(self, complete_graph, nodes_to_sample):\n",
    "        growth_size,T=2,100       # number of iterations (attempts to sample the graph)\n",
    "        complete_graph = nx.convert_node_labels_to_integers(complete_graph, 0, 'default', True)\n",
    "        # giving unique id to every node same as built-in function id\n",
    "        for n, data in complete_graph.nodes(data=True):\n",
    "            complete_graph.node[n]['id'] = n\n",
    "            \n",
    "        nr_nodes = len(complete_graph.nodes())\n",
    "        upper_bound_nr_nodes_to_sample = nodes_to_sample\n",
    "        index_of_first_random_node = random.randint(0, nr_nodes - 1)\n",
    "\n",
    "        Sampled_nodes = set([complete_graph.node[index_of_first_random_node]['id']])\n",
    "\n",
    "        iteration = 1\n",
    "        nodes_before_t_iter = 0\n",
    "        curr_node = index_of_first_random_node\n",
    "        while len(Sampled_nodes) != upper_bound_nr_nodes_to_sample:\n",
    "            edges = [n for n in complete_graph.neighbors(curr_node)]\n",
    "            index_of_edge = random.randint(0, len(edges) - 1)\n",
    "            chosen_node = edges[index_of_edge]\n",
    "            Sampled_nodes.add(complete_graph.node[chosen_node]['id'])\n",
    "            curr_node = chosen_node\n",
    "            iteration=iteration+1\n",
    "\n",
    "            if iteration % T == 0:\n",
    "                if ((len(Sampled_nodes) - nodes_before_t_iter) < growth_size):\n",
    "                    curr_node = random.randint(0, nr_nodes - 1)\n",
    "                nodes_before_t_iter = len(Sampled_nodes)\n",
    "\n",
    "        sampled_graph = complete_graph.subgraph(Sampled_nodes)\n",
    "\n",
    "        return sampled_graph\n",
    "    \n",
    "    def sample(self,G, samples_num):\n",
    "        for _ in range (samples_num):\n",
    "            if self.sampler_type==\"simple_random_sampling\": sampled_subgraph=self.simple_sampling(G,self.nodes_num)\n",
    "                \n",
    "            elif self.sampler_type==\"simple_random_walk_sampling\":\n",
    "                sampled_subgraph=self.random_walk_sampling_simple(G,self.nodes_num)\n",
    "\n",
    "            elif self.sampler_type==\"random_walk_flyback_sampling\":\n",
    "                sampled_subgraph=self.random_walk_sampling_with_fly_back(G,self.nodes_num,self.p_flyback)\n",
    "                \n",
    "            elif self.sampler_type==\"random_walk_induced_graph_sampling\":\n",
    "                sampled_subgraph=self.random_walk_induced_graph_sampling(G,self.nodes_num)\n",
    "          \n",
    "            adjacency=self.preprocess(nx.to_numpy_array(sampled_subgraph))[:,None] if _==0 \\\n",
    "                      else np.concatenate((adjacency,self.preprocess(nx.to_numpy_array(sampled_subgraph))[:,None]),axis=1)\n",
    "        return adjacency\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJt2mepm2tEx"
   },
   "outputs": [],
   "source": [
    "class feature_map(ABC):\n",
    "    '''\n",
    "    Abstract class for (random) feature mappings.\n",
    "    Ensure that the transform method is implemented.\n",
    "    '''\n",
    "    def __init__(self, input_dim, features_num):\n",
    "        self.input_dim=input_dim\n",
    "        self.output_dim=output_dim\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, A):\n",
    "        '''\n",
    "        In: A (input_dim * n)\n",
    "        Out: B (output_dim * n)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "class Gaussian_random_features(feature_map):\n",
    "    '''\n",
    "    This class affords an approximation of the Gaussian kernel using random features.\n",
    "    When initializing a new instance, you should pass: \n",
    "    sigma: STD of the Gaussian kernel\n",
    "    input_dim, features_num: size of projection matrix\n",
    "    '''\n",
    "    def __init__(self, input_dim, features_num, sigma):\n",
    "        self.proj_mat=sigma*np.random.randn(features_num,input_dim) \n",
    "        self.features_num=features_num\n",
    "\n",
    "    def transform(self, A):\n",
    "        temp = self.proj_mat.dot(A)\n",
    "        return np.concatenate((np.cos(temp),np.sin(temp)))\n",
    "\n",
    "    \n",
    "\n",
    "class Lighon_random_features(feature_map):\n",
    "    '''\n",
    "    This class affords an approximation of the Gaussian kernel using random features.\n",
    "    When initializing a new instance, you should pass: \n",
    "    sigma: STD of the Gaussian kernel\n",
    "    input_dim, features_num: size of projection matrix\n",
    "    '''\n",
    "    def __init__(self, input_dim, features_num):\n",
    "        self.features_num=features_num\n",
    "        self.random_mapping = OPUMap(n_components=features_num)\n",
    "\n",
    "    def transform(self, A):\n",
    "        A=np.uint8(A.T)\n",
    "        train_random_features = self.random_mapping.transform(A)\n",
    "        return train_random_features.astype('float32').T\n",
    "\n",
    "class graphlet_avg_features():\n",
    "    '''\n",
    "    Main class for graphlet (random) feature averaging.\n",
    "    Instanciated with a graph_sampler and a feature_map.\n",
    "    For each graph, graphlet sampling can be done by batch until samples_num is reached (by default, only one batch).\n",
    "    The graphlet size is implicitly contained in sampler and feat_map (of course, they should match)\n",
    "    '''\n",
    "    def __init__(self, samples_num, sampler, feat_map, batch_size=None, verbose=False):\n",
    "        if batch_size is None:\n",
    "            batch_size=samples_num\n",
    "        self.num_batches=int(samples_num/batch_size)\n",
    "        self.samples_num=self.num_batches*batch_size\n",
    "        self.batch_size=batch_size\n",
    "        self.sampler=sampler\n",
    "        self.feat_map=feat_map\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def calc_one_graph(self, G):\n",
    "        for _ in range(self.num_batches):\n",
    "            graphlets=self.sampler.sample(G, self.batch_size) # d*batch_size\n",
    "            random_feature=self.feat_map.transform(graphlets) # m*batch_size\n",
    "            result=random_feature.sum(axis=1) if _==0 \\\n",
    "                  else result + random_feature.sum(axis=1)\n",
    "        return result/self.samples_num\n",
    "\n",
    "    def apply(self, Gnx):\n",
    "        for (i,G) in enumerate(Gnx):\n",
    "            if self.verbose and np.mod(i,10)==0: print('Graph {}/{}'.format(i,len(Gnx)))\n",
    "            res=self.calc_one_graph(G)[:,None] if i==0 \\\n",
    "                else np.concatenate((res,self.calc_one_graph(G)[:,None]),axis=1)\n",
    "        return res\n",
    "    \n",
    "def calc_kernel(G_train, G_test, graphletRF):\n",
    "    random_features_train=graphletRF.apply(G_train)\n",
    "    random_features_test=graphletRF.apply(G_test)\n",
    "    K_train =random_features_train.T.dot(random_features_train)\n",
    "    K_test =random_features_test.T.dot(random_features_train)\n",
    "    return K_train, K_test\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAFcxRzg2tE3"
   },
   "source": [
    "\n",
    "# Dataset loading\n",
    "   ## Here you have three choices (mutag, SBM, DD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_loading:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    #mutag dataset\n",
    "    def mutag(self,test_size=0.1):\n",
    "        Gnx_train=[];\n",
    "        Gnx_test=[];\n",
    "        MUTAG = fetch_dataset(\"MUTAG\", verbose=False,as_graphs=False)\n",
    "        G, y = MUTAG.data, MUTAG.target\n",
    "        G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=0.1, random_state=42)\n",
    "        for i in range(len(G_train)):\n",
    "            g_current=nx.Graph(list(G_train[i][2]));\n",
    "            g_current.add_nodes_from(G_train[i][1])\n",
    "            Gnx_train.append(g_current)\n",
    "        for i in range(len(G_test)):\n",
    "            g_current=nx.Graph(list(G_test[i][2]));\n",
    "            g_current.add_nodes_from(G_test[i][1])\n",
    "            Gnx_test.append(g_current)\n",
    "        return (Gnx_train,y_train), (Gnx_test,y_test)\n",
    "\n",
    "    #SBM generator\n",
    "    def generate_SBM(self,Graphs_num=300,nodes_per_graph=60,block_size=10,fraction=0.3,mult_factor=1.2,avg_deg=10,test_size=0.2):\n",
    "        blocks_num=int(nodes_per_graph/block_size)\n",
    "        sizes=[block_size]*blocks_num\n",
    "        G,y=[],[]\n",
    "        for i in range (Graphs_num):                  \n",
    "            p_in=fraction  if i <Graphs_num/2 else fraction*mult_factor\n",
    "            p_out=(avg_deg-(block_size-1)*p_in)/(nodes_per_graph-block_size)\n",
    "            p=p_out*np.ones([blocks_num]*2)+(p_in-p_out)*np.eye(blocks_num)\n",
    "            #print(p_in,p_out)\n",
    "            G.append(nx.stochastic_block_model(sizes, p, seed=0))\n",
    "            y.append(-1 if i<Graphs_num/2 else 1)            \n",
    "        G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=test_size)\n",
    "        return (G_train,y_train),(G_test,y_test)\n",
    "        \n",
    "\n",
    "    # DD dataset\n",
    "    def DD(self,test_size=0.1,train_size=800):\n",
    "        DD = fetch_dataset(\"DD\", verbose=True)\n",
    "        G, y = DD.data, DD.target\n",
    "        Gnx_train=[];\n",
    "        Gnx_test=[];           # Taking just Train_size graphs of the data set as training set, \n",
    "                                       #this is due to the large computatational time\n",
    "        G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=test_size, random_state=42)\n",
    "        G_train,y_train=G_train[0:Train_size], y_train[0:Train_size]\n",
    "        for i in range(len(G_train)):\n",
    "            g_current=nx.Graph(list(G_train[i][0]));\n",
    "            g_current.add_nodes_from(G_train[i][1])\n",
    "            Gnx_train.append(g_current)\n",
    "        for i in range(len(G_test)):\n",
    "            g_current=nx.Graph(list(G_test[i][0]));\n",
    "            g_current.add_nodes_from(G_test[i][1])\n",
    "            Gnx_test.append(g_current)\n",
    "        return (Gnx_train,y_train), (Gnx_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.uint8(np.random.randint(0,2,[10,10]))\n",
    "random_mapping = OPUMap(n_components=10)\n",
    "train_random_features = random_mapping.transform(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c07afd6523f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGnx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"simple_random_sampling\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGnx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "Gnx=nx.complete_graph(20)\n",
    "sampler=graph_sampler(\"simple_random_sampling\",4)\n",
    "adj=sampler.sample(Gnx,5)\n",
    "print(adj.shape)\n",
    "Gaus=Gaussian_random_features(16,7,1)\n",
    "gaus_f=Gaus.transform(adj)\n",
    "print(gaus_f.shape)\n",
    "A=np.uint8(adj.T)\n",
    "random_mapping = OPUMap(n_components=7)\n",
    "train_random_features = random_mapping.transform(A)\n",
    "train_random_features= train_random_features.astype('float32').T\n",
    "#light=Lighon_random_features(16,7)\n",
    "#light_f=light.transform(adj)\n",
    "print(train_random_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf_ML_s82tFJ"
   },
   "source": [
    "\n",
    "\n",
    "# The following part includes setting up the hyperparameters and conducting the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the solution: model selection\n",
    "def run_grid(K_train, K_test, y_train, y_test, C_range = 10. ** np.arange(-2, 6)):\n",
    "    param_grid = dict(C=C_range)\n",
    "    grid = GridSearchCV(SVC(kernel='precomputed', gamma='auto'),\n",
    "                        param_grid=param_grid, cv=StratifiedKFold())\n",
    "    print('Fit...')\n",
    "    grid.fit(K_train, y_train)\n",
    "    # Training error\n",
    "    y_pred = grid.predict(K_test)\n",
    "\n",
    "    # Computes and prints the classification accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", str(round(acc*100, 2)) + \"%\")\n",
    "    return acc\n",
    "\n",
    "run_grid(K_train, K_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6379919,
     "status": "ok",
     "timestamp": 1584722880563,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "kcNEwc63PKPQ",
    "outputId": "e7ed865e-c40d-4957-f07f-9aa323c9bc21"
   },
   "outputs": [],
   "source": [
    "## test many features_num\n",
    "np.random.seed(0)\n",
    "nodes_num, samples_num, p_flyback, sigma= 6, 2000, 0.85, 10\n",
    "mult_factor=1.1 # the closer to 1, the harder the classification problem\n",
    "sampler_type=\"simple_random_sampling\"\n",
    "sampler=graph_sampler(sampler_type, nodes_num)\n",
    "######################################################\n",
    "feat_axis=[10, 25, 50, 100, 250, 500, 1000]\n",
    "num_expe=5\n",
    "result=np.zeros((len(feat_axis), num_expe))\n",
    "dataset=dataset_loading()\n",
    "for (f_ind, features_num) in enumerate(feat_axis):\n",
    "    for i in range(num_expe):\n",
    "        print('{}/{}, {}/{}'.format(f_ind+1, len(feat_axis), i+1, num_expe))\n",
    "        (G_train,y_train),(G_test,y_test) = dataset.generate_SBM(mult_factor=mult_factor, block_size=10) # generate a new synthetic dataset for each expe\n",
    "        feat_map=Gaussian_random_features(nodes_num**2, features_num, sigma) # generate a new RF for each expe\n",
    "        GRF = graphlet_avg_features(samples_num, sampler, feat_map, batch_size=None, verbose=True)\n",
    "        K_train,K_test=calc_kernel(G_train, G_test, GRF)\n",
    "        result[f_ind, i] = run_grid(K_train, K_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1584724061445,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "siMOqwxoWKJN",
    "outputId": "808eb743-bb99-49f8-e1df-461592fe77e7"
   },
   "outputs": [],
   "source": [
    "plt.semilogx(feat_axis, np.mean(result,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1584714964368,
     "user": {
      "displayName": "Nicolas Keriven",
      "photoUrl": "",
      "userId": "16022822615797753828"
     },
     "user_tz": -60
    },
    "id": "_CYz8Yl0Wuvz",
    "outputId": "3f67dcd2-a118-4353-8bfd-27b153d6b2b0"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "class_form.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
